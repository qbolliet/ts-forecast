{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - QB - Crossvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Importation des modules\n# Modules de base\nimport os\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom datetime import datetime, timedelta\nimport warnings\n\n# Configuration de l'affichage\nplt.style.use('seaborn-v0_8')\nsns.set_palette(\"husl\")\nnp.random.seed(42)  # Pour la reproductibilit√©\n\n# Importation des classes de validation crois√©e\nimport sys\nsys.path.append('../')\nfrom tsforecast.crossvals import (\n    TSOutOfSampleSplit, TSInSampleSplit,\n    PanelOutOfSampleSplit, PanelInSampleSplit,\n    PanelOutOfSampleSplitPerEntity, PanelInSampleSplitPerEntity\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## G√©n√©ration de donn√©es synth√©tiques\n\nCe notebook illustre l'utilisation des classes de validation crois√©e pour les s√©ries temporelles et les donn√©es de panel. Nous commen√ßons par g√©n√©rer des donn√©es synth√©tiques avec diff√©rentes caract√©ristiques pour d√©montrer le comportement des diff√©rentes m√©thodes."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468d49a",
   "metadata": {},
   "outputs": [],
   "source": "### 1. G√©n√©ration de s√©ries temporelles synth√©tiques\n\ndef generate_time_series(start_date='2020-01-01', periods=252, freq='D', trend_strength=0.02, \n                        seasonal_period=30, seasonal_strength=0.5, noise_std=1.0, \n                        ar_coefficient=0.7, add_outliers=False, outlier_prob=0.05):\n    \"\"\"\n    Generate synthetic time series with various patterns.\n    \n    Args:\n        start_date: Start date for the time series\n        periods: Number of time periods\n        freq: Frequency ('D' for daily, 'M' for monthly, etc.)\n        trend_strength: Strength of linear trend component\n        seasonal_period: Period of seasonal pattern\n        seasonal_strength: Strength of seasonal component\n        noise_std: Standard deviation of random noise\n        ar_coefficient: Autoregressive coefficient for AR(1) process\n        add_outliers: Whether to add random outliers\n        outlier_prob: Probability of outliers\n    \n    Returns:\n        pd.Series: Time series with DatetimeIndex\n    \"\"\"\n    # Cr√©ation de l'index temporel\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Composante tendance lin√©aire\n    trend = trend_strength * np.arange(periods)\n    \n    # Composante saisonni√®re\n    seasonal = seasonal_strength * np.sin(2 * np.pi * np.arange(periods) / seasonal_period)\n    \n    # Processus autor√©gressif AR(1) pour la persistance\n    ar_process = np.zeros(periods)\n    ar_process[0] = np.random.normal(0, noise_std)\n    for i in range(1, periods):\n        ar_process[i] = ar_coefficient * ar_process[i-1] + np.random.normal(0, noise_std)\n    \n    # Combinaison des composantes\n    values = trend + seasonal + ar_process\n    \n    # Ajout d'outliers al√©atoires\n    if add_outliers:\n        outlier_mask = np.random.random(periods) < outlier_prob\n        outlier_values = np.random.normal(0, 5 * noise_std, size=np.sum(outlier_mask))\n        values[outlier_mask] += outlier_values\n    \n    return pd.Series(values, index=dates, name='value')\n\n# G√©n√©ration de diff√©rents types de s√©ries temporelles\nprint(\"üìä G√©n√©ration de s√©ries temporelles avec diff√©rentes caract√©ristiques...\")\n\n# S√©rie 1: Trend fort, faible saisonnalit√©\nts_trend = generate_time_series(\n    start_date='2020-01-01', periods=200, freq='D',\n    trend_strength=0.05, seasonal_strength=0.2, noise_std=0.5,\n    ar_coefficient=0.8\n)\n\n# S√©rie 2: Saisonnalit√© forte, trend faible\nts_seasonal = generate_time_series(\n    start_date='2020-01-01', periods=200, freq='D',\n    trend_strength=0.01, seasonal_strength=1.5, seasonal_period=50,\n    noise_std=0.3, ar_coefficient=0.6\n)\n\n# S√©rie 3: S√©rie tr√®s bruit√©e avec outliers\nts_noisy = generate_time_series(\n    start_date='2020-01-01', periods=200, freq='D',\n    trend_strength=0.02, seasonal_strength=0.5, noise_std=2.0,\n    ar_coefficient=0.3, add_outliers=True, outlier_prob=0.08\n)\n\n# S√©rie 4: S√©rie stationnaire (pas de trend)\nts_stationary = generate_time_series(\n    start_date='2020-01-01', periods=200, freq='D',\n    trend_strength=0.0, seasonal_strength=0.8, noise_std=1.0,\n    ar_coefficient=0.5\n)\n\nprint(f\"‚úÖ G√©n√©ration termin√©e:\")\nprint(f\"  - S√©rie avec trend: {len(ts_trend)} observations de {ts_trend.index[0].date()} √† {ts_trend.index[-1].date()}\")\nprint(f\"  - S√©rie saisonni√®re: {len(ts_seasonal)} observations\")\nprint(f\"  - S√©rie bruit√©e: {len(ts_noisy)} observations\")\nprint(f\"  - S√©rie stationnaire: {len(ts_stationary)} observations\")"
  },
  {
   "cell_type": "code",
   "id": "qr9hepirsjr",
   "source": "# Visualisation des s√©ries temporelles g√©n√©r√©es\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('S√©ries temporelles synth√©tiques avec diff√©rentes caract√©ristiques', fontsize=16)\n\n# S√©rie avec trend\naxes[0, 0].plot(ts_trend.index, ts_trend.values, linewidth=1.5, color='blue')\naxes[0, 0].set_title('S√©rie avec trend fort')\naxes[0, 0].set_ylabel('Valeur')\naxes[0, 0].grid(True, alpha=0.3)\n\n# S√©rie saisonni√®re\naxes[0, 1].plot(ts_seasonal.index, ts_seasonal.values, linewidth=1.5, color='green')\naxes[0, 1].set_title('S√©rie avec saisonnalit√© forte')\naxes[0, 1].set_ylabel('Valeur')\naxes[0, 1].grid(True, alpha=0.3)\n\n# S√©rie bruit√©e\naxes[1, 0].plot(ts_noisy.index, ts_noisy.values, linewidth=1.5, color='red')\naxes[1, 0].set_title('S√©rie bruit√©e avec outliers')\naxes[1, 0].set_ylabel('Valeur')\naxes[1, 0].set_xlabel('Date')\naxes[1, 0].grid(True, alpha=0.3)\n\n# S√©rie stationnaire\naxes[1, 1].plot(ts_stationary.index, ts_stationary.values, linewidth=1.5, color='purple')\naxes[1, 1].set_title('S√©rie stationnaire')\naxes[1, 1].set_ylabel('Valeur')\naxes[1, 1].set_xlabel('Date')\naxes[1, 1].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "fdnrjsw9b6n",
   "source": "### 2. G√©n√©ration de donn√©es de panel synth√©tiques\n\nLes donn√©es de panel combinent plusieurs entit√©s observ√©es sur plusieurs p√©riodes temporelles. Nous allons cr√©er des datasets de panel avec diff√©rentes caract√©ristiques pour illustrer le comportement des classes de validation crois√©e.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "770bi4p2bpg",
   "source": "def generate_panel_data(entities=['A', 'B', 'C'], start_date='2020-01-01', periods=100, \n                        freq='D', heterogeneous_effects=True, common_trend=True, \n                        entity_specific_seasonality=True, cross_sectional_correlation=0.3,\n                        missing_data_prob=0.0):\n    \"\"\"\n    Generate synthetic panel data with various characteristics.\n    \n    Args:\n        entities: List of entity identifiers\n        start_date: Start date for the panel\n        periods: Number of time periods per entity\n        freq: Frequency of observations\n        heterogeneous_effects: Whether entities have different baseline levels\n        common_trend: Whether to include a common trend across entities\n        entity_specific_seasonality: Whether seasonality patterns differ by entity\n        cross_sectional_correlation: Correlation between entity shocks\n        missing_data_prob: Probability of missing observations\n    \n    Returns:\n        pd.DataFrame: Panel data with MultiIndex (entity, date)\n    \"\"\"\n    # Cr√©ation de l'index temporel\n    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n    \n    # Cr√©ation du MultiIndex (entity, date)\n    index = pd.MultiIndex.from_product([entities, dates], names=['entity', 'date'])\n    \n    # Initialisation du DataFrame\n    n_total = len(entities) * periods\n    panel_data = pd.DataFrame(index=index)\n    \n    # G√©n√©ration des effets fixes par entit√© (h√©t√©rog√©n√©it√©)\n    if heterogeneous_effects:\n        entity_effects = {entity: np.random.normal(0, 2) for entity in entities}\n    else:\n        entity_effects = {entity: 0 for entity in entities}\n    \n    # Tendance commune\n    if common_trend:\n        common_trend_values = 0.02 * np.arange(periods)\n    else:\n        common_trend_values = np.zeros(periods)\n    \n    # G√©n√©ration de chocs corr√©l√©s entre entit√©s\n    if cross_sectional_correlation > 0:\n        # Chocs communs\n        common_shocks = np.random.normal(0, 1, periods)\n        # Chocs idiosyncratiques\n        idiosyncratic_shocks = {\n            entity: np.random.normal(0, 1, periods) \n            for entity in entities\n        }\n    \n    # Construction des s√©ries pour chaque entit√©\n    values = []\n    entity_labels = []\n    date_labels = []\n    \n    for entity in entities:\n        # Effet fixe de l'entit√©\n        entity_effect = entity_effects[entity]\n        \n        # Saisonnalit√© sp√©cifique √† l'entit√©\n        if entity_specific_seasonality:\n            # P√©riode et amplitude diff√©rentes selon l'entit√©\n            seasonal_period = 20 + hash(entity) % 40  # Entre 20 et 60\n            seasonal_amplitude = 0.5 + (hash(entity) % 100) / 200  # Entre 0.5 et 1.0\n        else:\n            seasonal_period = 30\n            seasonal_amplitude = 0.5\n        \n        seasonal_values = seasonal_amplitude * np.sin(2 * np.pi * np.arange(periods) / seasonal_period)\n        \n        # Processus autor√©gressif sp√©cifique √† l'entit√©\n        ar_coef = 0.5 + (hash(entity) % 50) / 100  # Entre 0.5 et 1.0\n        ar_process = np.zeros(periods)\n        ar_process[0] = np.random.normal(0, 0.5)\n        for t in range(1, periods):\n            ar_process[t] = ar_coef * ar_process[t-1] + np.random.normal(0, 0.5)\n        \n        # Combinaison des composantes\n        if cross_sectional_correlation > 0:\n            # Chocs avec corr√©lation crois√©e\n            correlated_shocks = (\n                np.sqrt(cross_sectional_correlation) * common_shocks +\n                np.sqrt(1 - cross_sectional_correlation) * idiosyncratic_shocks[entity]\n            )\n        else:\n            correlated_shocks = np.random.normal(0, 1, periods)\n        \n        entity_values = (\n            entity_effect + \n            common_trend_values + \n            seasonal_values + \n            ar_process + \n            correlated_shocks\n        )\n        \n        # Ajout de donn√©es manquantes\n        if missing_data_prob > 0:\n            missing_mask = np.random.random(periods) < missing_data_prob\n            entity_values[missing_mask] = np.nan\n        \n        values.extend(entity_values)\n        entity_labels.extend([entity] * periods)\n        date_labels.extend(dates)\n    \n    # Cr√©ation du DataFrame final\n    panel_data['value'] = values\n    panel_data['entity'] = entity_labels\n    panel_data['date'] = date_labels\n    \n    # Ajout de variables explicatives\n    panel_data['lag_value'] = panel_data.groupby('entity')['value'].shift(1)\n    panel_data['trend'] = np.tile(np.arange(periods), len(entities))\n    panel_data['month'] = panel_data['date'].dt.month\n    \n    return panel_data[['value', 'lag_value', 'trend', 'month']]\n\n# G√©n√©ration de diff√©rents types de donn√©es de panel\nprint(\"üìä G√©n√©ration de donn√©es de panel avec diff√©rentes caract√©ristiques...\")\n\n# Panel 1: Donn√©es √©quilibr√©es avec effets h√©t√©rog√®nes\nentities_small = ['AAPL', 'GOOGL', 'MSFT', 'AMZN']\npanel_balanced = generate_panel_data(\n    entities=entities_small,\n    start_date='2020-01-01',\n    periods=120,\n    freq='D',\n    heterogeneous_effects=True,\n    common_trend=True,\n    entity_specific_seasonality=True,\n    cross_sectional_correlation=0.4\n)\n\n# Panel 2: Grand panel avec nombreuses entit√©s\nentities_large = [f'Entity_{i:02d}' for i in range(1, 21)]  # 20 entit√©s\npanel_large = generate_panel_data(\n    entities=entities_large,\n    start_date='2020-01-01',\n    periods=100,\n    freq='D',\n    heterogeneous_effects=True,\n    common_trend=True,\n    entity_specific_seasonality=False,  # Saisonnalit√© commune\n    cross_sectional_correlation=0.6\n)\n\n# Panel 3: Donn√©es avec observations manquantes\npanel_missing = generate_panel_data(\n    entities=['Entity_A', 'Entity_B', 'Entity_C'],\n    start_date='2020-01-01',\n    periods=80,\n    freq='D',\n    heterogeneous_effects=True,\n    common_trend=False,\n    entity_specific_seasonality=True,\n    cross_sectional_correlation=0.2,\n    missing_data_prob=0.05  # 5% de donn√©es manquantes\n)\n\nprint(f\"‚úÖ G√©n√©ration de panels termin√©e:\")\nprint(f\"  - Panel √©quilibr√©: {panel_balanced.shape[0]} observations, {len(entities_small)} entit√©s\")\nprint(f\"  - Grand panel: {panel_large.shape[0]} observations, {len(entities_large)} entit√©s\")\nprint(f\"  - Panel avec donn√©es manquantes: {panel_missing.shape[0]} observations, {panel_missing['value'].notna().sum()} valides\")\n\n# Affichage des premi√®res observations de chaque panel\nprint(f\"\\nüìã Aper√ßu des donn√©es:\")\nprint(f\"\\nPanel √©quilibr√© (premi√®res 10 observations):\")\nprint(panel_balanced.head(10))\nprint(f\"\\nPanel avec donn√©es manquantes (aper√ßu):\")\nprint(panel_missing[panel_missing['value'].isna()].head())",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jzdtjxksv88",
   "source": "# Visualisation des donn√©es de panel\nfig, axes = plt.subplots(2, 2, figsize=(15, 10))\nfig.suptitle('Donn√©es de panel synth√©tiques', fontsize=16)\n\n# Panel √©quilibr√© - quelques entit√©s\nentities_to_plot = entities_small[:3]\nfor i, entity in enumerate(entities_to_plot):\n    entity_data = panel_balanced.xs(entity, level='entity')\n    axes[0, 0].plot(entity_data.index, entity_data['value'], \n                   label=entity, linewidth=1.5, alpha=0.8)\naxes[0, 0].set_title('Panel √©quilibr√© (√©chantillon d\\'entit√©s)')\naxes[0, 0].set_ylabel('Valeur')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Distribution des valeurs par entit√© (panel √©quilibr√©)\npanel_balanced.reset_index().boxplot(column='value', by='entity', ax=axes[0, 1])\naxes[0, 1].set_title('Distribution par entit√© (panel √©quilibr√©)')\naxes[0, 1].set_ylabel('Valeur')\naxes[0, 1].set_xlabel('Entit√©')\n\n# Grand panel - moyennes par p√©riode\nlarge_panel_means = panel_large.groupby('date')['value'].agg(['mean', 'std']).reset_index()\naxes[1, 0].plot(large_panel_means['date'], large_panel_means['mean'], \n               color='blue', linewidth=1.5, label='Moyenne')\naxes[1, 0].fill_between(large_panel_means['date'], \n                       large_panel_means['mean'] - large_panel_means['std'],\n                       large_panel_means['mean'] + large_panel_means['std'],\n                       alpha=0.3, color='blue', label='¬±1 √©cart-type')\naxes[1, 0].set_title('Grand panel (20 entit√©s) - Statistiques agr√©g√©es')\naxes[1, 0].set_ylabel('Valeur')\naxes[1, 0].set_xlabel('Date')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Panel avec donn√©es manquantes\nmissing_stats = panel_missing.groupby('entity')['value'].apply(\n    lambda x: x.notna().sum() / len(x) * 100\n).reset_index()\nmissing_stats.columns = ['entity', 'completeness_pct']\nbars = axes[1, 1].bar(missing_stats['entity'], missing_stats['completeness_pct'])\naxes[1, 1].set_title('Compl√©tude des donn√©es par entit√© (%)')\naxes[1, 1].set_ylabel('Pourcentage de donn√©es valides')\naxes[1, 1].set_xlabel('Entit√©')\naxes[1, 1].set_ylim(0, 100)\n# Ajout des valeurs sur les barres\nfor bar, value in zip(bars, missing_stats['completeness_pct']):\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n                   f'{value:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "gznjj08803h",
   "source": "## D√©monstration des classes de validation crois√©e pour s√©ries temporelles\n\nLes classes `TSOutOfSampleSplit` et `TSInSampleSplit` sont sp√©cialement con√ßues pour les s√©ries temporelles. Elles respectent l'ordre temporel et permettent diff√©rentes configurations selon les besoins d'√©valuation.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "un6pkoksgv",
   "source": "### 3.1 TSOutOfSampleSplit - Validation hors √©chantillon\n\nLa validation **out-of-sample** (hors √©chantillon) est la m√©thode standard pour √©valuer les mod√®les de pr√©vision. L'entra√Ænement se fait **strictement sur le pass√©** et le test sur le **futur**, respectant ainsi l'ordre temporel naturel.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "0jluy1h3ywph",
   "source": "def analyze_split_characteristics(X, train_indices, test_indices, split_name):\n    \"\"\"Fonction utilitaire pour analyser les caract√©ristiques d'une s√©paration.\"\"\"\n    train_dates = X.index[train_indices]\n    test_dates = X.index[test_indices]\n    \n    print(f\"\\nüìä {split_name}:\")\n    print(f\"  - Taille d'entra√Ænement: {len(train_indices)} observations\")\n    print(f\"  - Taille de test: {len(test_indices)} observations\")\n    print(f\"  - P√©riode d'entra√Ænement: {train_dates[0].date()} √† {train_dates[-1].date()}\")\n    print(f\"  - P√©riode de test: {test_dates[0].date()} √† {test_dates[-1].date()}\")\n    \n    # V√©rification de l'ordre temporel\n    gap_days = (test_dates[0] - train_dates[-1]).days\n    print(f\"  - Gap entre train et test: {gap_days} jours\")\n    \n    return {\n        'train_size': len(train_indices),\n        'test_size': len(test_indices),\n        'train_start': train_dates[0],\n        'train_end': train_dates[-1],\n        'test_start': test_dates[0],\n        'test_end': test_dates[-1],\n        'gap_days': gap_days\n    }\n\nprint(\"üîç D√âMONSTRATION: TSOutOfSampleSplit avec diff√©rents param√®tres\")\nprint(\"=\"*70)\n\n# Utilisation de la s√©rie avec trend pour les d√©monstrations\nX = ts_trend.to_frame('value')\nprint(f\"S√©rie utilis√©e: {len(X)} observations de {X.index[0].date()} √† {X.index[-1].date()}\")\n\n# Configuration 1: Split basique avec n_splits\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 1: Split basique avec n_splits\")\nprint(f\"{'='*50}\")\n\nsplitter1 = TSOutOfSampleSplit(n_splits=3, test_size=20)\nsplits_info = []\n\nfor i, (train_idx, test_idx) in enumerate(splitter1.split(X)):\n    split_info = analyze_split_characteristics(X, train_idx, test_idx, f\"Split {i+1}\")\n    splits_info.append(split_info)\n\n# Configuration 2: Avec gap pour √©viter le data leakage\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 2: Avec gap pour √©viter le data leakage\")\nprint(f\"{'='*50}\")\n\nsplitter2 = TSOutOfSampleSplit(n_splits=3, test_size=15, gap=5)\nprint(\"‚ö†Ô∏è  Gap = 5 jours entre l'entra√Ænement et le test\")\n\nfor i, (train_idx, test_idx) in enumerate(splitter2.split(X)):\n    analyze_split_characteristics(X, train_idx, test_idx, f\"Split avec gap {i+1}\")\n\n# Configuration 3: Fen√™tre d'entra√Ænement limit√©e (rolling window)\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 3: Fen√™tre d'entra√Ænement limit√©e (rolling window)\")\nprint(f\"{'='*50}\")\n\nsplitter3 = TSOutOfSampleSplit(n_splits=3, test_size=15, max_train_size=50, gap=2)\nprint(\"üìè max_train_size = 50 observations (fen√™tre glissante)\")\n\nfor i, (train_idx, test_idx) in enumerate(splitter3.split(X)):\n    analyze_split_characteristics(X, train_idx, test_idx, f\"Rolling window {i+1}\")\n\n# Configuration 4: Test sur des dates sp√©cifiques\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 4: Test sur des dates sp√©cifiques\")\nprint(f\"{'='*50}\")\n\nspecific_test_dates = ['2020-03-01', '2020-04-15', '2020-06-01']\nsplitter4 = TSOutOfSampleSplit(test_indices=specific_test_dates, test_size=10, gap=3)\nprint(f\"üéØ Dates de test sp√©cifiques: {specific_test_dates}\")\n\nfor i, (train_idx, test_idx) in enumerate(splitter4.split(X)):\n    analyze_split_characteristics(X, train_idx, test_idx, f\"Test sp√©cifique {i+1}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "jl0n1mg5qmc",
   "source": "# Visualisation des diff√©rentes configurations de TSOutOfSampleSplit\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle('Visualisation des diff√©rentes configurations TSOutOfSampleSplit', fontsize=16)\n\nconfigurations = [\n    (splitter1, \"Split basique (n_splits=3)\", axes[0, 0]),\n    (splitter2, \"Avec gap=5\", axes[0, 1]), \n    (splitter3, \"Fen√™tre limit√©e (max_train_size=50)\", axes[1, 0]),\n    (splitter4, \"Dates sp√©cifiques\", axes[1, 1])\n]\n\ncolors = ['blue', 'red', 'green', 'orange', 'purple']\n\nfor splitter, title, ax in configurations:\n    # Plot de la s√©rie compl√®te\n    ax.plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n    \n    # Plot des splits\n    for i, (train_idx, test_idx) in enumerate(splitter.split(X)):\n        train_data = X.iloc[train_idx]\n        test_data = X.iloc[test_idx]\n        \n        # Donn√©es d'entra√Ænement\n        ax.plot(train_data.index, train_data['value'], \n               color=colors[i], alpha=0.7, linewidth=2, \n               label=f'Train {i+1}' if i < 3 else None)\n        \n        # Donn√©es de test\n        ax.scatter(test_data.index, test_data['value'], \n                  color=colors[i], s=30, alpha=0.9, marker='o',\n                  edgecolors='black', linewidth=0.5,\n                  label=f'Test {i+1}' if i < 3 else None)\n    \n    ax.set_title(title)\n    ax.set_ylabel('Valeur')\n    ax.grid(True, alpha=0.3)\n    if ax in [axes[1, 0], axes[1, 1]]:\n        ax.set_xlabel('Date')\n    \n    # L√©gende seulement pour le premier graphique\n    if ax == axes[0, 0]:\n        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n\nplt.tight_layout()\nplt.show()\n\n# R√©sum√© des caract√©ristiques importantes\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìã R√âSUM√â DES CARACT√âRISTIQUES IMPORTANTES\")\nprint(\"=\"*70)\nprint(\"\\n‚úÖ Points cl√©s √† retenir:\")\nprint(\"  1. Out-of-sample: l'entra√Ænement pr√©c√®de TOUJOURS le test temporellement\")\nprint(\"  2. Gap: permet d'√©viter le data leakage en laissant un intervalle\")\nprint(\"  3. max_train_size: limite la fen√™tre d'entra√Ænement (rolling window)\")\nprint(\"  4. test_indices: permet de tester sur des p√©riodes sp√©cifiques\")\nprint(\"  5. Les splits respectent l'ordre chronologique des donn√©es\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "3jofqtfrcdk",
   "source": "### 3.2 TSInSampleSplit - Validation dans l'√©chantillon\n\nLa validation **in-sample** (dans l'√©chantillon) inclut la p√©riode de test dans les donn√©es d'entra√Ænement. Cette approche est utile pour l'√©valuation historique et la calibration de mod√®les, o√π l'information future est disponible.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "tw8nthfoue",
   "source": "print(\"üîç D√âMONSTRATION: TSInSampleSplit - Validation dans l'√©chantillon\")\nprint(\"=\"*70)\n\ndef analyze_insample_split(X, train_indices, test_indices, split_name):\n    \"\"\"Fonction pour analyser les splits in-sample.\"\"\"\n    train_dates = X.index[train_indices]\n    test_dates = X.index[test_indices]\n    \n    print(f\"\\nüìä {split_name}:\")\n    print(f\"  - Taille d'entra√Ænement: {len(train_indices)} observations\")\n    print(f\"  - Taille de test: {len(test_indices)} observations\")\n    print(f\"  - P√©riode d'entra√Ænement: {train_dates[0].date()} √† {train_dates[-1].date()}\")\n    print(f\"  - P√©riode de test: {test_dates[0].date()} √† {test_dates[-1].date()}\")\n    \n    # V√©rification que le test est inclus dans l'entra√Ænement\n    test_in_train = all(idx in train_indices for idx in test_indices)\n    print(f\"  - Test inclus dans train: {'‚úÖ Oui' if test_in_train else '‚ùå Non'}\")\n    \n    return test_in_train\n\n# Configuration 1: In-sample basique\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 1: In-sample basique\")\nprint(f\"{'='*50}\")\n\ninsample_splitter1 = TSInSampleSplit(test_size=20)\nprint(\"üìñ Les donn√©es de test sont incluses dans l'entra√Ænement\")\n\nfor i, (train_idx, test_idx) in enumerate(insample_splitter1.split(X)):\n    analyze_insample_split(X, train_idx, test_idx, f\"In-sample split {i+1}\")\n\n# Configuration 2: In-sample avec fen√™tre d'entra√Ænement limit√©e\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 2: In-sample avec max_train_size\")\nprint(f\"{'='*50}\")\n\ninsample_splitter2 = TSInSampleSplit(test_size=15, max_train_size=80)\nprint(\"üìè Entra√Ænement limit√© mais inclut toujours la p√©riode de test\")\n\nfor i, (train_idx, test_idx) in enumerate(insample_splitter2.split(X)):\n    analyze_insample_split(X, train_idx, test_idx, f\"Limited in-sample {i+1}\")\n\n# Configuration 3: In-sample sur dates sp√©cifiques\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 3: In-sample sur dates sp√©cifiques\")\nprint(f\"{'='*50}\")\n\nspecific_dates = ['2020-04-01']\ninsample_splitter3 = TSInSampleSplit(test_indices=specific_dates, test_size=14)\nprint(f\"üéØ Test sur p√©riode sp√©cifique: {specific_dates[0]} (14 jours)\")\n\nfor i, (train_idx, test_idx) in enumerate(insample_splitter3.split(X)):\n    analyze_insample_split(X, train_idx, test_idx, f\"Specific in-sample {i+1}\")\n\n# Configuration 4: Comparaison de plusieurs dates sp√©cifiques\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 4: Multiples dates sp√©cifiques\")\nprint(f\"{'='*50}\")\n\nmultiple_dates = ['2020-02-15', '2020-03-15', '2020-05-01']\ninsample_splitter4 = TSInSampleSplit(test_indices=multiple_dates, test_size=7)\nprint(f\"üéØ Tests sur: {multiple_dates}\")\n\nfor i, (train_idx, test_idx) in enumerate(insample_splitter4.split(X)):\n    analyze_insample_split(X, train_idx, test_idx, f\"Multiple dates {i+1}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "og19pb6ryk9",
   "source": "# Visualisation comparative: Out-of-sample vs In-sample\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\nfig.suptitle('Comparaison Out-of-sample vs In-sample', fontsize=16)\n\n# Donn√©es de test communes pour la comparaison\ntest_date_comparison = '2020-04-01'\ntest_size_comparison = 10\n\n# Out-of-sample\nout_splitter = TSOutOfSampleSplit(test_indices=[test_date_comparison], test_size=test_size_comparison, gap=2)\nin_splitter = TSInSampleSplit(test_indices=[test_date_comparison], test_size=test_size_comparison)\n\n# Visualisation Out-of-sample\nfor train_idx, test_idx in out_splitter.split(X):\n    train_data = X.iloc[train_idx]\n    test_data = X.iloc[test_idx]\n    \n    axes[0, 0].plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n    axes[0, 0].plot(train_data.index, train_data['value'], color='blue', alpha=0.8, linewidth=2, label='Train')\n    axes[0, 0].scatter(test_data.index, test_data['value'], color='red', s=40, alpha=0.9, \n                      edgecolors='black', linewidth=0.5, label='Test')\n    \n    # Mise en √©vidence du gap\n    if len(train_data) > 0 and len(test_data) > 0:\n        gap_start = train_data.index[-1]\n        gap_end = test_data.index[0]\n        axes[0, 0].axvspan(gap_start, gap_end, alpha=0.3, color='yellow', label='Gap')\n\naxes[0, 0].set_title('Out-of-sample: Train ‚Üí Gap ‚Üí Test')\naxes[0, 0].set_ylabel('Valeur')\naxes[0, 0].legend()\naxes[0, 0].grid(True, alpha=0.3)\n\n# Visualisation In-sample\nfor train_idx, test_idx in in_splitter.split(X):\n    train_data = X.iloc[train_idx]\n    test_data = X.iloc[test_idx]\n    \n    axes[0, 1].plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n    axes[0, 1].plot(train_data.index, train_data['value'], color='blue', alpha=0.8, linewidth=2, label='Train')\n    axes[0, 1].scatter(test_data.index, test_data['value'], color='red', s=40, alpha=0.9, \n                      edgecolors='black', linewidth=0.5, label='Test')\n    \n    # Mise en √©vidence de l'overlap\n    overlap_indices = np.intersect1d(train_idx, test_idx)\n    if len(overlap_indices) > 0:\n        overlap_data = X.iloc[overlap_indices]\n        axes[0, 1].scatter(overlap_data.index, overlap_data['value'], color='purple', s=60, \n                          alpha=0.7, marker='s', edgecolors='black', linewidth=1, label='Overlap (Test dans Train)')\n\naxes[0, 1].set_title('In-sample: Test inclus dans Train')\naxes[0, 1].set_ylabel('Valeur')\naxes[0, 1].legend()\naxes[0, 1].grid(True, alpha=0.3)\n\n# Graphique de distribution des erreurs simul√©es\nprint(\"\\nüßÆ Simulation d'√©valuation avec diff√©rents mod√®les...\")\n\n# Simulation d'erreurs pour d√©montrer l'impact\nnp.random.seed(42)\nn_simulations = 1000\n\n# Erreurs out-of-sample (plus r√©alistes)\nout_sample_errors = np.random.normal(0, 1.5, n_simulations)  # Plus d'incertitude\n\n# Erreurs in-sample (g√©n√©ralement plus faibles)\nin_sample_errors = np.random.normal(0, 0.8, n_simulations)   # Moins d'incertitude\n\naxes[1, 0].hist(out_sample_errors, bins=50, alpha=0.7, color='red', label='Out-of-sample', density=True)\naxes[1, 0].hist(in_sample_errors, bins=50, alpha=0.7, color='blue', label='In-sample', density=True)\naxes[1, 0].set_title('Distribution des erreurs de pr√©diction')\naxes[1, 0].set_xlabel('Erreur')\naxes[1, 0].set_ylabel('Densit√©')\naxes[1, 0].legend()\naxes[1, 0].grid(True, alpha=0.3)\n\n# Statistiques comparatives\nout_mae = np.mean(np.abs(out_sample_errors))\nin_mae = np.mean(np.abs(in_sample_errors))\nout_mse = np.mean(out_sample_errors**2)\nin_mse = np.mean(in_sample_errors**2)\n\nmetrics = ['MAE', 'MSE']\nout_values = [out_mae, out_mse]\nin_values = [in_mae, in_mse]\n\nx = np.arange(len(metrics))\nwidth = 0.35\n\nbars1 = axes[1, 1].bar(x - width/2, out_values, width, label='Out-of-sample', color='red', alpha=0.7)\nbars2 = axes[1, 1].bar(x + width/2, in_values, width, label='In-sample', color='blue', alpha=0.7)\n\naxes[1, 1].set_title('M√©triques d\\'erreur comparatives')\naxes[1, 1].set_ylabel('Valeur de l\\'erreur')\naxes[1, 1].set_xticks(x)\naxes[1, 1].set_xticklabels(metrics)\naxes[1, 1].legend()\naxes[1, 1].grid(True, alpha=0.3)\n\n# Ajout des valeurs sur les barres\nfor bar in bars1:\n    height = bar.get_height()\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n                   f'{height:.3f}', ha='center', va='bottom')\nfor bar in bars2:\n    height = bar.get_height()\n    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n                   f'{height:.3f}', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"üìã COMPARAISON OUT-OF-SAMPLE vs IN-SAMPLE\")\nprint(\"=\"*70)\nprint(\"\\nüìä R√©sultats simul√©s:\")\nprint(f\"  Out-of-sample MAE: {out_mae:.3f}\")\nprint(f\"  In-sample MAE: {in_mae:.3f}\")\nprint(f\"  Diff√©rence: {((out_mae - in_mae) / in_mae * 100):+.1f}%\")\nprint(\"\\n‚úÖ Points cl√©s:\")\nprint(\"  1. Out-of-sample: √©valuation r√©aliste de la capacit√© pr√©dictive\")\nprint(\"  2. In-sample: √©valuation optimiste, utile pour la calibration\")\nprint(\"  3. L'√©cart refl√®te le challenge r√©el de la pr√©diction\")\nprint(\"  4. In-sample inclut information future ‚Üí erreurs plus faibles\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "tdqht99wpn",
   "source": "## D√©monstration des classes de validation crois√©e pour donn√©es de panel\n\nLes donn√©es de panel combinent plusieurs entit√©s observ√©es dans le temps. Les classes `PanelOutOfSampleSplit` et `PanelInSampleSplit` g√®rent cette complexit√© en appliquant la logique de validation √† chaque entit√© tout en permettant l'agr√©gation des r√©sultats.",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "id": "fxbvksunyn",
   "source": "### 4.1 PanelOutOfSampleSplit - Validation hors √©chantillon pour donn√©es de panel\n\nCette classe applique la logique out-of-sample √† chaque entit√© du panel, respectant l'ordre temporel au sein de chaque entit√©.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "7zxxnn6b3r",
   "source": "def analyze_panel_split(X, train_indices, test_indices, split_name, max_entities_display=5):\n    \"\"\"Fonction pour analyser les splits de panel.\"\"\"\n    # Extraction des entit√©s pr√©sentes dans train et test\n    train_entities = X.iloc[train_indices].index.get_level_values('entity').unique()\n    test_entities = X.iloc[test_indices].index.get_level_values('entity').unique()\n    \n    print(f\"\\nüìä {split_name}:\")\n    print(f\"  - Observations d'entra√Ænement: {len(train_indices)}\")\n    print(f\"  - Observations de test: {len(test_indices)}\")\n    print(f\"  - Entit√©s dans train: {len(train_entities)} {list(train_entities[:max_entities_display])}\")\n    print(f\"  - Entit√©s dans test: {len(test_entities)} {list(test_entities[:max_entities_display])}\")\n    \n    # Analyse par entit√©\n    if len(test_entities) <= max_entities_display:\n        for entity in test_entities:\n            entity_train = X.iloc[train_indices].xs(entity, level='entity', drop_level=False)\n            entity_test = X.iloc[test_indices].xs(entity, level='entity', drop_level=False)\n            \n            if len(entity_train) > 0 and len(entity_test) > 0:\n                train_dates = entity_train.index.get_level_values('date')\n                test_dates = entity_test.index.get_level_values('date')\n                gap_days = (test_dates[0] - train_dates[-1]).days\n                print(f\"    {entity}: Train {train_dates[0].date()}‚Üí{train_dates[-1].date()}, Test {test_dates[0].date()}‚Üí{test_dates[-1].date()}, Gap {gap_days}j\")\n\nprint(\"üîç D√âMONSTRATION: PanelOutOfSampleSplit\")\nprint(\"=\"*70)\n\n# Utilisation du panel √©quilibr√© pour les d√©monstrations\nX_panel = panel_balanced[['value']]\nprint(f\"Panel utilis√©: {X_panel.shape[0]} observations, {len(X_panel.index.get_level_values('entity').unique())} entit√©s\")\nprint(f\"P√©riode: {X_panel.index.get_level_values('date').min().date()} √† {X_panel.index.get_level_values('date').max().date()}\")\n\n# Configuration 1: Split basique avec n_splits\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 1: Panel out-of-sample basique\")\nprint(f\"{'='*50}\")\n\npanel_splitter1 = PanelOutOfSampleSplit(n_splits=3, test_size=10)\nprint(\"üìä Validation crois√©e avec 3 splits, 10 observations de test par entit√©\")\n\nfor i, (train_idx, test_idx) in enumerate(panel_splitter1.split(X_panel)):\n    analyze_panel_split(X_panel, train_idx, test_idx, f\"Panel split {i+1}\")\n    if i >= 2:  # Limiter l'affichage\n        break\n\n# Configuration 2: Avec gap\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 2: Panel avec gap\")\nprint(f\"{'='*50}\")\n\npanel_splitter2 = PanelOutOfSampleSplit(n_splits=2, test_size=8, gap=5)\nprint(\"‚ö†Ô∏è  Gap de 5 jours entre train et test pour chaque entit√©\")\n\nfor i, (train_idx, test_idx) in enumerate(panel_splitter2.split(X_panel)):\n    analyze_panel_split(X_panel, train_idx, test_idx, f\"Panel avec gap {i+1}\")\n\n# Configuration 3: Test sur dates sp√©cifiques\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 3: Test sur dates sp√©cifiques (panel)\")\nprint(f\"{'='*50}\")\n\nspecific_panel_dates = ['2020-03-01', '2020-04-15']\npanel_splitter3 = PanelOutOfSampleSplit(test_indices=specific_panel_dates, test_size=7, gap=2)\nprint(f\"üéØ Tests sur: {specific_panel_dates} pour toutes les entit√©s\")\n\nfor i, (train_idx, test_idx) in enumerate(panel_splitter3.split(X_panel)):\n    analyze_panel_split(X_panel, train_idx, test_idx, f\"Dates sp√©cifiques {i+1}\")\n\n# Configuration 4: Fen√™tre d'entra√Ænement limit√©e\nprint(f\"\\n{'='*50}\")\nprint(\"CONFIGURATION 4: Fen√™tre d'entra√Ænement limit√©e (panel)\")\nprint(f\"{'='*50}\")\n\npanel_splitter4 = PanelOutOfSampleSplit(n_splits=2, test_size=6, max_train_size=30, gap=1)\nprint(\"üìè max_train_size = 30 observations par entit√©\")\n\nfor i, (train_idx, test_idx) in enumerate(panel_splitter4.split(X_panel)):\n    analyze_panel_split(X_panel, train_idx, test_idx, f\"Rolling window panel {i+1}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "h67e6ffpqd6",
   "source": "# Visualisation des splits de panel\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\nfig.suptitle('Visualisation des splits PanelOutOfSampleSplit', fontsize=16)\n\n# Configuration pour la visualisation\nentities_to_plot = entities_small[:3]  # Premi√®re 3 entit√©s pour la clart√©\ncolors_entities = ['blue', 'red', 'green']\n\n# Configuration 1: Split basique\nax = axes[0, 0]\nfor i, (train_idx, test_idx) in enumerate(panel_splitter1.split(X_panel)):\n    if i > 0:  # Seulement le premier split pour la clart√©\n        break\n    \n    for j, entity in enumerate(entities_to_plot):\n        try:\n            # Donn√©es compl√®tes de l'entit√©\n            entity_data = X_panel.xs(entity, level='entity')\n            ax.plot(entity_data.index, entity_data['value'], \n                   color=colors_entities[j], alpha=0.3, linewidth=1, \n                   label=f'{entity} (complet)' if i == 0 else \"\")\n            \n            # Donn√©es d'entra√Ænement\n            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_train_data) > 0:\n                train_dates = entity_train_data.index.get_level_values('date')\n                ax.plot(train_dates, entity_train_data['value'], \n                       color=colors_entities[j], alpha=0.8, linewidth=2,\n                       label=f'{entity} Train' if i == 0 else \"\")\n            \n            # Donn√©es de test\n            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_test_data) > 0:\n                test_dates = entity_test_data.index.get_level_values('date')\n                ax.scatter(test_dates, entity_test_data['value'], \n                          color=colors_entities[j], s=40, alpha=0.9,\n                          edgecolors='black', linewidth=0.5,\n                          label=f'{entity} Test' if i == 0 else \"\")\n        except:\n            pass\n\nax.set_title('Panel split basique')\nax.set_ylabel('Valeur')\nax.grid(True, alpha=0.3)\nax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n\n# Configuration 2: Avec gap\nax = axes[0, 1]\nfor i, (train_idx, test_idx) in enumerate(panel_splitter2.split(X_panel)):\n    if i > 0:  # Seulement le premier split\n        break\n    \n    for j, entity in enumerate(entities_to_plot):\n        try:\n            entity_data = X_panel.xs(entity, level='entity')\n            ax.plot(entity_data.index, entity_data['value'], \n                   color=colors_entities[j], alpha=0.3, linewidth=1)\n            \n            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_train_data) > 0:\n                train_dates = entity_train_data.index.get_level_values('date')\n                ax.plot(train_dates, entity_train_data['value'], \n                       color=colors_entities[j], alpha=0.8, linewidth=2)\n            \n            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_test_data) > 0:\n                test_dates = entity_test_data.index.get_level_values('date')\n                ax.scatter(test_dates, entity_test_data['value'], \n                          color=colors_entities[j], s=40, alpha=0.9,\n                          edgecolors='black', linewidth=0.5)\n                \n                # Visualisation du gap\n                if len(entity_train_data) > 0:\n                    gap_start = train_dates[-1]\n                    gap_end = test_dates[0]\n                    ax.axvspan(gap_start, gap_end, alpha=0.2, color='yellow')\n        except:\n            pass\n\nax.set_title('Panel avec gap=5')\nax.set_ylabel('Valeur')\nax.grid(True, alpha=0.3)\n\n# Configuration 3: Dates sp√©cifiques\nax = axes[1, 0]\nfor i, (train_idx, test_idx) in enumerate(panel_splitter3.split(X_panel)):\n    if i > 1:  # Limite √† 2 splits\n        break\n    \n    for j, entity in enumerate(entities_to_plot):\n        try:\n            entity_data = X_panel.xs(entity, level='entity')\n            ax.plot(entity_data.index, entity_data['value'], \n                   color=colors_entities[j], alpha=0.3, linewidth=1)\n            \n            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_train_data) > 0:\n                train_dates = entity_train_data.index.get_level_values('date')\n                ax.plot(train_dates, entity_train_data['value'], \n                       color=colors_entities[j], alpha=0.8, linewidth=2)\n            \n            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_test_data) > 0:\n                test_dates = entity_test_data.index.get_level_values('date')\n                ax.scatter(test_dates, entity_test_data['value'], \n                          color=colors_entities[j], s=40, alpha=0.9,\n                          edgecolors='black', linewidth=0.5,\n                          marker='s' if i == 0 else 'o')\n        except:\n            pass\n\nax.set_title('Dates sp√©cifiques')\nax.set_ylabel('Valeur')\nax.set_xlabel('Date')\nax.grid(True, alpha=0.3)\n\n# Configuration 4: Fen√™tre limit√©e\nax = axes[1, 1]\nfor i, (train_idx, test_idx) in enumerate(panel_splitter4.split(X_panel)):\n    if i > 0:  # Seulement le premier split\n        break\n    \n    for j, entity in enumerate(entities_to_plot):\n        try:\n            entity_data = X_panel.xs(entity, level='entity')\n            ax.plot(entity_data.index, entity_data['value'], \n                   color=colors_entities[j], alpha=0.3, linewidth=1)\n            \n            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_train_data) > 0:\n                train_dates = entity_train_data.index.get_level_values('date')\n                ax.plot(train_dates, entity_train_data['value'], \n                       color=colors_entities[j], alpha=0.8, linewidth=2)\n            \n            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n            if len(entity_test_data) > 0:\n                test_dates = entity_test_data.index.get_level_values('date')\n                ax.scatter(test_dates, entity_test_data['value'], \n                          color=colors_entities[j], s=40, alpha=0.9,\n                          edgecolors='black', linewidth=0.5)\n        except:\n            pass\n\nax.set_title('Fen√™tre limit√©e (max_train_size=30)')\nax.set_ylabel('Valeur')\nax.set_xlabel('Date')\nax.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "zvm4mjf6rxq",
   "source": "### 4.2 Classes sp√©cialis√©es pour le traitement par entit√©\n\nLes classes `PanelOutOfSampleSplitPerEntity` et `PanelInSampleSplitPerEntity` permettent de traiter chaque entit√© s√©par√©ment, ce qui est utile pour l'analyse individuelle et le traitement parall√®le.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "13yshjeq25zq",
   "source": "print(\"üîç D√âMONSTRATION: Classes sp√©cialis√©es par entit√©\")\nprint(\"=\"*70)\n\n# Comparaison des approches PanelOutOfSampleSplit vs PanelOutOfSampleSplitPerEntity\nprint(\"\\nüìä Comparaison: Agr√©g√© vs Par entit√©\")\nprint(\"-\" * 50)\n\n# Splitter agr√©g√© (standard)\npanel_agg_splitter = PanelOutOfSampleSplit(n_splits=2, test_size=5, gap=2)\n\n# Splitter par entit√©\npanel_per_entity_splitter = PanelOutOfSampleSplitPerEntity(n_splits=2, test_size=5, gap=2)\n\nprint(\"\\n1Ô∏è‚É£  APPROCHE AGR√âG√âE (PanelOutOfSampleSplit):\")\nsplit_count = 0\nfor train_idx, test_idx in panel_agg_splitter.split(X_panel):\n    split_count += 1\n    entities_in_test = X_panel.iloc[test_idx].index.get_level_values('entity').unique()\n    print(f\"  Split {split_count}: {len(test_idx)} observations de test, {len(entities_in_test)} entit√©s\")\n    if split_count >= 2:\n        break\n\nprint(f\"\\n  üìà Total: {split_count} splits avec toutes les entit√©s m√©lang√©es\")\n\nprint(\"\\n2Ô∏è‚É£  APPROCHE PAR ENTIT√â (PanelOutOfSampleSplitPerEntity):\")\nsplit_count = 0\nentity_splits = {}\n\nfor train_idx, test_idx in panel_per_entity_splitter.split(X_panel):\n    split_count += 1\n    # Identifier l'entit√© de ce split\n    test_entity = X_panel.iloc[test_idx].index.get_level_values('entity').unique()[0]\n    train_entity = X_panel.iloc[train_idx].index.get_level_values('entity').unique()[0] if len(train_idx) > 0 else \"N/A\"\n    \n    if test_entity not in entity_splits:\n        entity_splits[test_entity] = 0\n    entity_splits[test_entity] += 1\n    \n    print(f\"  Split {split_count}: Entit√© {test_entity}, {len(train_idx)} train, {len(test_idx)} test\")\n    \n    if split_count >= 8:  # Limiter l'affichage\n        print(\"  ... (splits suppl√©mentaires)\")\n        break\n\nprint(f\"\\n  üìà Total: {split_count}+ splits individuels par entit√©\")\nprint(f\"  üìä R√©partition par entit√©: {dict(entity_splits)}\")\n\n# D√©monstration avec In-sample per entity\nprint(f\"\\n{'='*50}\")\nprint(\"D√âMONSTRATION: PanelInSampleSplitPerEntity\")\nprint(f\"{'='*50}\")\n\npanel_insample_per_entity = PanelInSampleSplitPerEntity(test_indices=['2020-03-01'], test_size=7)\n\nprint(\"üéØ Test sur 2020-03-01 avec validation in-sample par entit√©:\")\nentity_results = {}\n\nfor train_idx, test_idx in panel_insample_per_entity.split(X_panel):\n    test_entity = X_panel.iloc[test_idx].index.get_level_values('entity').unique()[0]\n    train_data = X_panel.iloc[train_idx]\n    test_data = X_panel.iloc[test_idx]\n    \n    # V√©rification que le test est inclus dans l'entra√Ænement\n    test_in_train = all(idx in train_idx for idx in test_idx)\n    \n    entity_results[test_entity] = {\n        'train_size': len(train_idx),\n        'test_size': len(test_idx),\n        'test_in_train': test_in_train,\n        'train_period': (train_data.index.get_level_values('date').min().date(), \n                        train_data.index.get_level_values('date').max().date()),\n        'test_period': (test_data.index.get_level_values('date').min().date(),\n                       test_data.index.get_level_values('date').max().date())\n    }\n    \n    print(f\"  {test_entity}: Train {entity_results[test_entity]['train_size']} obs, \"\n          f\"Test {entity_results[test_entity]['test_size']} obs, \"\n          f\"Test in Train: {'‚úÖ' if test_in_train else '‚ùå'}\")\n\n# Statistiques de comparaison\nprint(f\"\\n{'='*50}\")\nprint(\"üìä STATISTIQUES COMPARATIVES\")\nprint(f\"{'='*50}\")\n\ndef calculate_split_statistics(splitter, X, split_type_name):\n    \"\"\"Calcule des statistiques sur les splits.\"\"\"\n    total_splits = 0\n    total_train_obs = 0\n    total_test_obs = 0\n    entities_seen = set()\n    \n    for train_idx, test_idx in splitter.split(X):\n        total_splits += 1\n        total_train_obs += len(train_idx)\n        total_test_obs += len(test_idx)\n        \n        test_entities = X.iloc[test_idx].index.get_level_values('entity').unique()\n        entities_seen.update(test_entities)\n        \n        if total_splits >= 10:  # Limite pour √©viter trop de calculs\n            break\n    \n    return {\n        'type': split_type_name,\n        'total_splits': total_splits,\n        'avg_train_size': total_train_obs / total_splits if total_splits > 0 else 0,\n        'avg_test_size': total_test_obs / total_splits if total_splits > 0 else 0,\n        'unique_entities': len(entities_seen)\n    }\n\n# Calcul des statistiques\nstats_agg = calculate_split_statistics(panel_agg_splitter, X_panel, \"Agr√©g√©\")\nstats_per_entity = calculate_split_statistics(panel_per_entity_splitter, X_panel, \"Par entit√©\")\n\nprint(f\"\\nüìà R√©sultats (sur {min(10, stats_agg['total_splits'])} premiers splits):\")\nprint(f\"  Approche agr√©g√©e:\")\nprint(f\"    - Splits: {stats_agg['total_splits']}\")\nprint(f\"    - Taille moyenne train: {stats_agg['avg_train_size']:.1f}\")\nprint(f\"    - Taille moyenne test: {stats_agg['avg_test_size']:.1f}\")\nprint(f\"    - Entit√©s uniques vues: {stats_agg['unique_entities']}\")\n\nprint(f\"  Approche par entit√©:\")\nprint(f\"    - Splits: {stats_per_entity['total_splits']}\")\nprint(f\"    - Taille moyenne train: {stats_per_entity['avg_train_size']:.1f}\")\nprint(f\"    - Taille moyenne test: {stats_per_entity['avg_test_size']:.1f}\")\nprint(f\"    - Entit√©s uniques vues: {stats_per_entity['unique_entities']}\")\n\nprint(f\"\\n‚úÖ Avantages par approche:\")\nprint(f\"  üìä Agr√©g√©e: Moins de splits, √©valuation globale, plus rapide\")\nprint(f\"  üéØ Par entit√©: Analyse d√©taill√©e, traitement parall√®le possible, contr√¥le granulaire\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "6v7uwfe7pyg",
   "source": "## Guide pratique et meilleures pratiques\n\nCette section pr√©sente un guide pratique pour choisir la m√©thode de validation crois√©e appropri√©e selon diff√©rents sc√©narios et objectifs d'analyse.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "id": "hcgbssic1li",
   "source": "print(\"üìö GUIDE PRATIQUE: Choisir la bonne m√©thode de validation crois√©e\")\nprint(\"=\"*80)\n\n# Matrice de d√©cision\ndecision_matrix = {\n    \"Type de donn√©es\": {\n        \"S√©rie temporelle unique\": [\"TSOutOfSampleSplit\", \"TSInSampleSplit\"],\n        \"Panel data (multi-entit√©s)\": [\"PanelOutOfSampleSplit\", \"PanelInSampleSplit\", \n                                      \"PanelOutOfSampleSplitPerEntity\", \"PanelInSampleSplitPerEntity\"]\n    },\n    \"Objectif d'√©valuation\": {\n        \"Pr√©diction future (production)\": [\"OutOfSampleSplit\"],\n        \"Analyse historique/calibration\": [\"InSampleSplit\"],\n        \"√âvaluation comparative\": [\"OutOfSampleSplit\", \"InSampleSplit\"]\n    },\n    \"Contraintes temporelles\": {\n        \"√âviter data leakage\": [\"gap > 0\"],\n        \"Fen√™tre glissante\": [\"max_train_size < inf\"],\n        \"Dates sp√©cifiques\": [\"test_indices=[dates]\"]\n    },\n    \"Analyse par entit√©\": {\n        \"√âvaluation globale\": [\"PanelOutOfSampleSplit\", \"PanelInSampleSplit\"],\n        \"Analyse individuelle\": [\"PanelOutOfSampleSplitPerEntity\", \"PanelInSampleSplitPerEntity\"],\n        \"Traitement parall√®le\": [\"PerEntity classes\"]\n    }\n}\\n\\n# Affichage du guide de d√©cision\\nfor category, options in decision_matrix.items():\\n    print(f\\\"\\\\nüéØ {category.upper()}:\\\")\\n    for scenario, methods in options.items():\\n        print(f\\\"  ‚Ä¢ {scenario}: {', '.join(methods)}\\\")\\n\\n# Exemples de configurations recommand√©es\\nprint(f\\\"\\\\n{'='*50}\\\")\\nprint(\\\"CONFIGURATIONS RECOMMAND√âES\\\")\\nprint(f\\\"{'='*50}\\\")\\n\\nrecommended_configs = [\\n    {\\n        \\\"scenario\\\": \\\"Pr√©diction de s√©ries temporelles financi√®res\\\",\\n        \\\"config\\\": \\\"TSOutOfSampleSplit(n_splits=5, test_size=22, gap=1)\\\",\\n        \\\"rationale\\\": \\\"Gap d'1 jour pour √©viter le look-ahead bias, test sur ~1 mois\\\"\\n    },\\n    {\\n        \\\"scenario\\\": \\\"√âvaluation de mod√®les sur panel d'entreprises\\\",\\n        \\\"config\\\": \\\"PanelOutOfSampleSplit(test_size=30, gap=5, max_train_size=252)\\\",\\n        \\\"rationale\\\": \\\"Fen√™tre d'1 an, gap de 5 jours, test sur 1 mois\\\"\\n    },\\n    {\\n        \\\"scenario\\\": \\\"Backtesting historique avec calibration\\\",\\n        \\\"config\\\": \\\"TSInSampleSplit(test_indices=['2020-03-01'], test_size=14)\\\",\\n        \\\"rationale\\\": \\\"Test sur p√©riode de crise sp√©cifique, entra√Ænement inclut le futur\\\"\\n    },\\n    {\\n        \\\"scenario\\\": \\\"Analyse de robustesse par entit√©\\\",\\n        \\\"config\\\": \\\"PanelOutOfSampleSplitPerEntity(n_splits=3, test_size=10)\\\",\\n        \\\"rationale\\\": \\\"√âvaluation individuelle de chaque entit√© avec 3 p√©riodes de test\\\"\\n    },\\n    {\\n        \\\"scenario\\\": \\\"Validation avec donn√©es haute fr√©quence\\\",\\n        \\\"config\\\": \\\"TSOutOfSampleSplit(test_size=50, gap=10, max_train_size=1000)\\\",\\n        \\\"rationale\\\": \\\"Gap plus important, fen√™tre limit√©e pour donn√©es intraday\\\"\\n    }\\n]\\n\\nfor i, config in enumerate(recommended_configs, 1):\\n    print(f\\\"\\\\n{i}Ô∏è‚É£  {config['scenario']}:\\\")\\n    print(f\\\"   üìù Configuration: {config['config']}\\\")\\n    print(f\\\"   üí° Rationale: {config['rationale']}\\\")\\n\\n# M√©triques de performance simul√©es\\nprint(f\\\"\\\\n{'='*50}\\\")\\nprint(\\\"EXEMPLE DE PIPELINE D'√âVALUATION\\\")\\nprint(f\\\"{'='*50}\\\")\\n\\ndef simulate_model_evaluation(X, splitter, model_name=\\\"Mod√®le simple\\\"):\\n    \\\"\\\"\\\"Simule l'√©valuation d'un mod√®le avec cross-validation.\\\"\\\"\\\"\\n    mae_scores = []\\n    mse_scores = []\\n    \\n    split_count = 0\\n    for train_idx, test_idx in splitter.split(X):\\n        # Simulation d'entra√Ænement et pr√©diction\\n        X_train = X.iloc[train_idx]\\n        X_test = X.iloc[test_idx]\\n        \\n        # Pr√©diction naive (moyenne mobile) pour simulation\\n        if len(X_train) > 0:\\n            if hasattr(X_train.index, 'get_level_values'):\\n                # Panel data - moyenne par groupe\\n                pred = X_train.groupby(level='entity')['value'].mean().mean()\\n            else:\\n                # S√©rie temporelle - moyenne des derni√®res valeurs\\n                pred = X_train['value'].tail(min(10, len(X_train))).mean()\\n        else:\\n            pred = 0\\n        \\n        # Calcul des erreurs simul√©es\\n        true_values = X_test['value'].values\\n        predictions = np.full_like(true_values, pred)\\n        \\n        # Ajout de bruit r√©aliste selon le type de validation\\n        if 'OutOfSample' in splitter.__class__.__name__:\\n            noise_std = 0.8  # Plus d'incertitude out-of-sample\\n        else:\\n            noise_std = 0.4  # Moins d'incertitude in-sample\\n        \\n        predictions += np.random.normal(0, noise_std, len(predictions))\\n        \\n        mae = np.mean(np.abs(true_values - predictions))\\n        mse = np.mean((true_values - predictions)**2)\\n        \\n        mae_scores.append(mae)\\n        mse_scores.append(mse)\\n        \\n        split_count += 1\\n        if split_count >= 5:  # Limite pour la d√©monstration\\n            break\\n    \\n    return {\\n        'mae_mean': np.mean(mae_scores),\\n        'mae_std': np.std(mae_scores),\\n        'mse_mean': np.mean(mse_scores),\\n        'mse_std': np.std(mse_scores),\\n        'n_splits': len(mae_scores)\\n    }\\n\\n# Comparaison de performance entre diff√©rentes m√©thodes\\nprint(\\\"\\\\nüî¨ Simulation d'√©valuation comparative:\\\")\\n\\nsplitters_to_compare = [\\n    (TSOutOfSampleSplit(n_splits=3, test_size=15, gap=2), \\\"TS Out-of-Sample\\\"),\\n    (TSInSampleSplit(test_size=15), \\\"TS In-Sample\\\"),\\n    (PanelOutOfSampleSplit(n_splits=2, test_size=8), \\\"Panel Out-of-Sample\\\"),\\n    (PanelInSampleSplit(test_size=8), \\\"Panel In-Sample\\\")\\n]\\n\\nresults = []\\nfor splitter, name in splitters_to_compare:\\n    if 'Panel' in name:\\n        data = X_panel\\n    else:\\n        data = X\\n    \\n    try:\\n        result = simulate_model_evaluation(data, splitter, name)\\n        result['method'] = name\\n        results.append(result)\\n        \\n        print(f\\\"\\\\n  üìä {name}:\\\")\\n        print(f\\\"     MAE: {result['mae_mean']:.3f} ¬± {result['mae_std']:.3f}\\\")\\n        print(f\\\"     MSE: {result['mse_mean']:.3f} ¬± {result['mse_std']:.3f}\\\")\\n        print(f\\\"     Splits: {result['n_splits']}\\\")\\n    except Exception as e:\\n        print(f\\\"\\\\n  ‚ùå {name}: Erreur - {str(e)[:50]}...\\\")\\n\\nprint(f\\\"\\\\n{'='*50}\\\")\\nprint(\\\"POINTS CL√âS √Ä RETENIR\\\")\\nprint(f\\\"{'='*50}\\\")\\n\\nkey_points = [\\n    \\\"Out-of-sample donne une √©valuation plus conservative et r√©aliste\\\",\\n    \\\"In-sample est utile pour l'analyse historique et la calibration\\\",\\n    \\\"Le gap pr√©vient le data leakage dans les donn√©es haute fr√©quence\\\",\\n    \\\"max_train_size permet une validation en fen√™tre glissante\\\",\\n    \\\"Les classes Panel g√®rent automatiquement la structure multi-entit√©s\\\",\\n    \\\"PerEntity permet l'analyse granulaire et le traitement parall√®le\\\",\\n    \\\"test_indices permet de tester sur des √©v√©nements sp√©cifiques\\\"\\n]\\n\\nfor i, point in enumerate(key_points, 1):\\n    print(f\\\"  {i}. {point}\\\")\\n\\nprint(f\\\"\\\\n‚úÖ Le choix de la m√©thode d√©pend de votre objectif:\\\")\\nprint(f\\\"   üéØ Production: Out-of-sample avec gap appropri√©\\\")\\nprint(f\\\"   üìä Recherche: In-sample pour analyse historique\\\")\\nprint(f\\\"   üî¨ Robustesse: Comparaison des deux approches\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "d84m3mwzzm",
   "source": "## Conclusion\n\nCe notebook a pr√©sent√© de mani√®re exhaustive les classes de validation crois√©e du module `tsforecast.crossvals`. Voici les points essentiels √† retenir :\n\n### Fonctionnalit√©s principales\n\n1. **Classes pour s√©ries temporelles** : `TSOutOfSampleSplit` et `TSInSampleSplit`\n   - Respectent l'ordre chronologique\n   - G√®rent les gaps pour √©viter le data leakage\n   - Supportent les fen√™tres glissantes et les dates sp√©cifiques\n\n2. **Classes pour donn√©es de panel** : `PanelOutOfSampleSplit`, `PanelInSampleSplit` et leurs variantes `PerEntity`\n   - Traitent les donn√©es multi-entit√©s automatiquement\n   - Appliquent la logique temporelle au sein de chaque entit√©\n   - Permettent l'analyse granulaire par entit√©\n\n3. **Flexibilit√© de configuration** : \n   - Param√®tres `n_splits`, `test_size`, `gap`, `max_train_size`\n   - Support des `test_indices` pour des p√©riodes sp√©cifiques\n   - Compatible avec l'API sklearn\n\n### Recommandations d'usage\n\n- **Out-of-sample** : Pour l'√©valuation r√©aliste de mod√®les de production\n- **In-sample** : Pour l'analyse historique et la calibration\n- **Gap** : Essentiel pour les donn√©es haute fr√©quence\n- **PerEntity** : Pour l'analyse d√©taill√©e et le traitement parall√®le\n\nCes classes offrent une base solide pour l'√©valuation rigoureuse de mod√®les sur donn√©es temporelles et de panel, en respectant les contraintes inh√©rentes √† ce type de donn√©es.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-forecast-YhvOQP7D-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}