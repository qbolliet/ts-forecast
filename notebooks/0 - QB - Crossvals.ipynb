{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - QB - Crossvals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des modules\n",
    "# Modules de base\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "# Configuration de l'affichage\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "np.random.seed(42)  # Pour la reproductibilit√©\n",
    "\n",
    "# Importation des classes de validation crois√©e\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from tsforecast.crossvals import (\n",
    "    TSOutOfSampleSplit, TSInSampleSplit,\n",
    "    PanelOutOfSampleSplit, PanelInSampleSplit,\n",
    "    PanelOutOfSampleSplitPerEntity, PanelInSampleSplitPerEntity\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## G√©n√©ration de donn√©es synth√©tiques\n",
    "\n",
    "Ce notebook illustre l'utilisation des classes de validation crois√©e pour les s√©ries temporelles et les donn√©es de panel. Nous commen√ßons par g√©n√©rer des donn√©es synth√©tiques avec diff√©rentes caract√©ristiques pour d√©montrer le comportement des diff√©rentes m√©thodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7468d49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de g√©n√©ration de s√©ries temporelles\n",
    "def generate_time_series(start_date='2010-01-01', periods=180, freq='MS', trend_strength=0.02, \n",
    "                        seasonal_period=30, seasonal_strength=0.5, noise_std=1.0, \n",
    "                        ar_coefficient=0.7, add_outliers=False, outlier_prob=0.05):\n",
    "    \"\"\"\n",
    "    Generate synthetic time series with various patterns.\n",
    "    \n",
    "    Args:\n",
    "        start_date: Start date for the time series\n",
    "        periods: Number of time periods\n",
    "        freq: Frequency ('D' for daily, 'M' for monthly, etc.)\n",
    "        trend_strength: Strength of linear trend component\n",
    "        seasonal_period: Period of seasonal pattern\n",
    "        seasonal_strength: Strength of seasonal component\n",
    "        noise_std: Standard deviation of random noise\n",
    "        ar_coefficient: Autoregressive coefficient for AR(1) process\n",
    "        add_outliers: Whether to add random outliers\n",
    "        outlier_prob: Probability of outliers\n",
    "    \n",
    "    Returns:\n",
    "        pd.Series: Time series with DatetimeIndex\n",
    "    \"\"\"\n",
    "    # Cr√©ation de l'index temporel\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq, inclusive=\"both\")\n",
    "    \n",
    "    # Composante tendance lin√©aire\n",
    "    trend = trend_strength * np.arange(periods)\n",
    "    \n",
    "    # Composante saisonni√®re\n",
    "    seasonal = seasonal_strength * np.sin(2 * np.pi * np.arange(periods) / seasonal_period)\n",
    "    \n",
    "    # Processus autor√©gressif AR(1) pour la persistance\n",
    "    ar_process = np.zeros(periods)\n",
    "    ar_process[0] = np.random.normal(0, noise_std)\n",
    "    for i in range(1, periods):\n",
    "        ar_process[i] = ar_coefficient * ar_process[i-1] + np.random.normal(0, noise_std)\n",
    "    \n",
    "    # Combinaison des composantes\n",
    "    values = trend + seasonal + ar_process\n",
    "    \n",
    "    # Ajout d'outliers al√©atoires\n",
    "    if add_outliers:\n",
    "        outlier_mask = np.random.random(periods) < outlier_prob\n",
    "        outlier_values = np.random.normal(0, 5 * noise_std, size=np.sum(outlier_mask))\n",
    "        values[outlier_mask] += outlier_values\n",
    "    \n",
    "    return pd.Series(values, index=dates, name='value')\n",
    "\n",
    "# G√©n√©ration de diff√©rents types de s√©ries temporelles\n",
    "print(\"üìä G√©n√©ration de s√©ries temporelles avec diff√©rentes caract√©ristiques...\")\n",
    "\n",
    "# S√©rie 1: Trend fort, faible saisonnalit√©\n",
    "ts_trend = generate_time_series(\n",
    "    start_date='2010-01-01', periods=180, freq='MS',\n",
    "    trend_strength=0.05, seasonal_strength=0.2, noise_std=0.5,\n",
    "    ar_coefficient=0.8\n",
    ")\n",
    "\n",
    "# S√©rie 2: Saisonnalit√© forte, trend faible\n",
    "ts_seasonal = generate_time_series(\n",
    "    start_date='2010-01-01', periods=180, freq='MS',\n",
    "    trend_strength=0.01, seasonal_strength=1.5, seasonal_period=50,\n",
    "    noise_std=0.3, ar_coefficient=0.6\n",
    ")\n",
    "\n",
    "# S√©rie 3: S√©rie tr√®s bruit√©e avec outliers\n",
    "ts_noisy = generate_time_series(\n",
    "    start_date='2010-01-01', periods=180, freq='MS',\n",
    "    trend_strength=0.02, seasonal_strength=0.5, noise_std=2.0,\n",
    "    ar_coefficient=0.3, add_outliers=True, outlier_prob=0.08\n",
    ")\n",
    "\n",
    "# S√©rie 4: S√©rie stationnaire (pas de trend)\n",
    "ts_stationary = generate_time_series(\n",
    "    start_date='2010-01-01', periods=180, freq='MS',\n",
    "    trend_strength=0.0, seasonal_strength=0.8, noise_std=1.0,\n",
    "    ar_coefficient=0.5\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ G√©n√©ration termin√©e:\")\n",
    "print(f\"  - S√©rie avec trend: {len(ts_trend)} observations de {ts_trend.index[0].date()} √† {ts_trend.index[-1].date()}\")\n",
    "print(f\"  - S√©rie saisonni√®re: {len(ts_seasonal)} observations\")\n",
    "print(f\"  - S√©rie bruit√©e: {len(ts_noisy)} observations\")\n",
    "print(f\"  - S√©rie stationnaire: {len(ts_stationary)} observations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qr9hepirsjr",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des s√©ries temporelles g√©n√©r√©es\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('S√©ries temporelles synth√©tiques avec diff√©rentes caract√©ristiques', fontsize=16)\n",
    "\n",
    "# S√©rie avec trend\n",
    "axes[0, 0].plot(ts_trend.index, ts_trend.values, linewidth=1.5, color='blue')\n",
    "axes[0, 0].set_title('S√©rie avec trend fort')\n",
    "axes[0, 0].set_ylabel('Valeur')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# S√©rie saisonni√®re\n",
    "axes[0, 1].plot(ts_seasonal.index, ts_seasonal.values, linewidth=1.5, color='green')\n",
    "axes[0, 1].set_title('S√©rie avec saisonnalit√© forte')\n",
    "axes[0, 1].set_ylabel('Valeur')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# S√©rie bruit√©e\n",
    "axes[1, 0].plot(ts_noisy.index, ts_noisy.values, linewidth=1.5, color='red')\n",
    "axes[1, 0].set_title('S√©rie bruit√©e avec outliers')\n",
    "axes[1, 0].set_ylabel('Valeur')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# S√©rie stationnaire\n",
    "axes[1, 1].plot(ts_stationary.index, ts_stationary.values, linewidth=1.5, color='purple')\n",
    "axes[1, 1].set_title('S√©rie stationnaire')\n",
    "axes[1, 1].set_ylabel('Valeur')\n",
    "axes[1, 1].set_xlabel('Date')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdnrjsw9b6n",
   "metadata": {},
   "source": [
    "### 2. G√©n√©ration de donn√©es de panel synth√©tiques\n",
    "\n",
    "Les donn√©es de panel combinent plusieurs entit√©s observ√©es sur plusieurs p√©riodes temporelles. Nous allons cr√©er des datasets de panel avec diff√©rentes caract√©ristiques pour illustrer le comportement des classes de validation crois√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770bi4p2bpg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction de g√©n√©ration des donn√©es de panel\n",
    "def generate_panel_data(entities=['A', 'B', 'C'], start_date='2015-01-01', periods=120, \n",
    "                        freq='MS', heterogeneous_effects=True, common_trend=True, \n",
    "                        entity_specific_seasonality=True, cross_sectional_correlation=0.3,\n",
    "                        missing_data_prob=0.0):\n",
    "    \"\"\"\n",
    "    Generate synthetic panel data with various characteristics.\n",
    "    \n",
    "    Args:\n",
    "        entities: List of entity identifiers\n",
    "        start_date: Start date for the panel\n",
    "        periods: Number of time periods per entity\n",
    "        freq: Frequency of observations\n",
    "        heterogeneous_effects: Whether entities have different baseline levels\n",
    "        common_trend: Whether to include a common trend across entities\n",
    "        entity_specific_seasonality: Whether seasonality patterns differ by entity\n",
    "        cross_sectional_correlation: Correlation between entity shocks\n",
    "        missing_data_prob: Probability of missing observations\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Panel data with MultiIndex (entity, date)\n",
    "    \"\"\"\n",
    "    # Cr√©ation de l'index temporel\n",
    "    dates = pd.date_range(start=start_date, periods=periods, freq=freq)\n",
    "    \n",
    "    # Cr√©ation du MultiIndex (entity, date)\n",
    "    index = pd.MultiIndex.from_product([entities, dates], names=['entity', 'date'])\n",
    "    \n",
    "    # Initialisation du DataFrame\n",
    "    panel_data = pd.DataFrame(index=index)\n",
    "    \n",
    "    # G√©n√©ration des effets fixes par entit√© (h√©t√©rog√©n√©it√©)\n",
    "    if heterogeneous_effects:\n",
    "        entity_effects = {entity: np.random.normal(0, 2) for entity in entities}\n",
    "    else:\n",
    "        entity_effects = {entity: 0 for entity in entities}\n",
    "    \n",
    "    # Tendance commune\n",
    "    if common_trend:\n",
    "        common_trend_values = 0.02 * np.arange(periods)\n",
    "    else:\n",
    "        common_trend_values = np.zeros(periods)\n",
    "    \n",
    "    # G√©n√©ration de chocs corr√©l√©s entre entit√©s\n",
    "    if cross_sectional_correlation > 0:\n",
    "        # Chocs communs\n",
    "        common_shocks = np.random.normal(0, 1, periods)\n",
    "        # Chocs idiosyncratiques\n",
    "        idiosyncratic_shocks = {\n",
    "            entity: np.random.normal(0, 1, periods) \n",
    "            for entity in entities\n",
    "        }\n",
    "    \n",
    "    # Construction des s√©ries pour chaque entit√©\n",
    "    values = []\n",
    "    \n",
    "    # Parcours des entit√©s\n",
    "    for entity in entities:\n",
    "        # Effet fixe de l'entit√©\n",
    "        entity_effect = entity_effects[entity]\n",
    "        \n",
    "        # Saisonnalit√© sp√©cifique √† l'entit√©\n",
    "        if entity_specific_seasonality:\n",
    "            # P√©riode et amplitude diff√©rentes selon l'entit√©\n",
    "            seasonal_period = 20 + hash(entity) % 40  # Entre 20 et 60\n",
    "            seasonal_amplitude = 0.5 + (hash(entity) % 100) / 200  # Entre 0.5 et 1.0\n",
    "        else:\n",
    "            seasonal_period = 30\n",
    "            seasonal_amplitude = 0.5\n",
    "        \n",
    "        seasonal_values = seasonal_amplitude * np.sin(2 * np.pi * np.arange(periods) / seasonal_period)\n",
    "        \n",
    "        # Processus autor√©gressif sp√©cifique √† l'entit√©\n",
    "        ar_coef = 0.5 + (hash(entity) % 50) / 100  # Entre 0.5 et 1.0\n",
    "        ar_process = np.zeros(periods)\n",
    "        ar_process[0] = np.random.normal(0, 0.5)\n",
    "        for t in range(1, periods):\n",
    "            ar_process[t] = ar_coef * ar_process[t-1] + np.random.normal(0, 0.5)\n",
    "        \n",
    "        # Combinaison des composantes\n",
    "        if cross_sectional_correlation > 0:\n",
    "            # Chocs avec corr√©lation crois√©e\n",
    "            correlated_shocks = (\n",
    "                np.sqrt(cross_sectional_correlation) * common_shocks +\n",
    "                np.sqrt(1 - cross_sectional_correlation) * idiosyncratic_shocks[entity]\n",
    "            )\n",
    "        else:\n",
    "            correlated_shocks = np.random.normal(0, 1, periods)\n",
    "        \n",
    "        # Combinaison des valeurs\n",
    "        entity_values = (\n",
    "            entity_effect + \n",
    "            common_trend_values + \n",
    "            seasonal_values + \n",
    "            ar_process + \n",
    "            correlated_shocks\n",
    "        )\n",
    "        \n",
    "        # Ajout de donn√©es manquantes\n",
    "        if missing_data_prob > 0:\n",
    "            missing_mask = np.random.random(periods) < missing_data_prob\n",
    "            entity_values[missing_mask] = np.nan\n",
    "        \n",
    "        values.extend(entity_values)\n",
    "    \n",
    "    # Cr√©ation du DataFrame final avec les valeurs\n",
    "    panel_data['value'] = values\n",
    "    \n",
    "    # Ajout de variables explicatives\n",
    "    panel_data['lag_value'] = panel_data.groupby('entity')['value'].shift(1)\n",
    "    panel_data['trend'] = np.tile(np.arange(periods), len(entities))\n",
    "    panel_data['month'] = panel_data.index.get_level_values('date').month\n",
    "    \n",
    "    return panel_data\n",
    "\n",
    "# G√©n√©ration de diff√©rents types de donn√©es de panel\n",
    "print(\"üìä G√©n√©ration de donn√©es de panel avec diff√©rentes caract√©ristiques...\")\n",
    "\n",
    "# Panel 1: Donn√©es √©quilibr√©es avec effets h√©t√©rog√®nes\n",
    "entities_small = ['FR', 'DE', 'IT', 'ES']\n",
    "panel_balanced = generate_panel_data(\n",
    "    entities=entities_small,\n",
    "    start_date='2015-01-01',\n",
    "    periods=120,\n",
    "    freq='MS',\n",
    "    heterogeneous_effects=True,\n",
    "    common_trend=True,\n",
    "    entity_specific_seasonality=True,\n",
    "    cross_sectional_correlation=0.4\n",
    ")\n",
    "\n",
    "# Panel 2: Grand panel avec nombreuses entit√©s\n",
    "entities_large = [f'Entity_{i:02d}' for i in range(1, 21)]  # 20 entit√©s\n",
    "panel_large = generate_panel_data(\n",
    "    entities=entities_large,\n",
    "    start_date='2015-01-01',\n",
    "    periods=120,\n",
    "    freq='MS',\n",
    "    heterogeneous_effects=True,\n",
    "    common_trend=True,\n",
    "    entity_specific_seasonality=False,  # Saisonnalit√© commune\n",
    "    cross_sectional_correlation=0.6\n",
    ")\n",
    "\n",
    "# Panel 3: Donn√©es avec observations manquantes\n",
    "panel_missing = generate_panel_data(\n",
    "    entities=['Entity_A', 'Entity_B', 'Entity_C'],\n",
    "    start_date='2020-01-01',\n",
    "    periods=60,\n",
    "    freq='MS',\n",
    "    heterogeneous_effects=True,\n",
    "    common_trend=False,\n",
    "    entity_specific_seasonality=True,\n",
    "    cross_sectional_correlation=0.2,\n",
    "    missing_data_prob=0.05  # 5% de donn√©es manquantes\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ G√©n√©ration de panels termin√©e:\")\n",
    "print(f\"  - Panel √©quilibr√©: {panel_balanced.shape[0]} observations, {len(entities_small)} entit√©s\")\n",
    "print(f\"  - Grand panel: {panel_large.shape[0]} observations, {len(entities_large)} entit√©s\")\n",
    "print(f\"  - Panel avec donn√©es manquantes: {panel_missing.shape[0]} observations, {panel_missing['value'].notna().sum()} valides\")\n",
    "\n",
    "# Affichage des premi√®res observations de chaque panel\n",
    "print(f\"\\nüìã Aper√ßu des donn√©es:\")\n",
    "print(f\"\\nPanel √©quilibr√© (premi√®res 10 observations):\")\n",
    "print(panel_balanced.head(10))\n",
    "print(f\"\\nPanel avec donn√©es manquantes (aper√ßu):\")\n",
    "print(panel_missing[panel_missing['value'].isna()].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jzdtjxksv88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des donn√©es de panel\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Donn√©es de panel synth√©tiques', fontsize=16)\n",
    "\n",
    "# Panel √©quilibr√© - quelques entit√©s\n",
    "entities_to_plot = entities_small[:3]\n",
    "for i, entity in enumerate(entities_to_plot):\n",
    "    entity_data = panel_balanced.xs(entity, level='entity')\n",
    "    axes[0, 0].plot(entity_data.index, entity_data['value'], \n",
    "                   label=entity, linewidth=1.5, alpha=0.8)\n",
    "axes[0, 0].set_title('Panel √©quilibr√© (√©chantillon d\\'entit√©s)')\n",
    "axes[0, 0].set_ylabel('Valeur')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des valeurs par entit√© (panel √©quilibr√©)\n",
    "panel_balanced.reset_index().boxplot(column='value', by='entity', ax=axes[0, 1])\n",
    "axes[0, 1].set_title('Distribution par entit√© (panel √©quilibr√©)')\n",
    "axes[0, 1].set_ylabel('Valeur')\n",
    "axes[0, 1].set_xlabel('Entit√©')\n",
    "\n",
    "# Grand panel - moyennes par p√©riode\n",
    "large_panel_means = panel_large.groupby('date')['value'].agg(['mean', 'std']).reset_index()\n",
    "axes[1, 0].plot(large_panel_means['date'], large_panel_means['mean'], \n",
    "               color='blue', linewidth=1.5, label='Moyenne')\n",
    "axes[1, 0].fill_between(large_panel_means['date'], \n",
    "                       large_panel_means['mean'] - large_panel_means['std'],\n",
    "                       large_panel_means['mean'] + large_panel_means['std'],\n",
    "                       alpha=0.3, color='blue', label='¬±1 √©cart-type')\n",
    "axes[1, 0].set_title('Grand panel (20 entit√©s) - Statistiques agr√©g√©es')\n",
    "axes[1, 0].set_ylabel('Valeur')\n",
    "axes[1, 0].set_xlabel('Date')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Panel avec donn√©es manquantes\n",
    "missing_stats = panel_missing.groupby('entity')['value'].apply(\n",
    "    lambda x: x.notna().sum() / len(x) * 100\n",
    ").reset_index()\n",
    "missing_stats.columns = ['entity', 'completeness_pct']\n",
    "bars = axes[1, 1].bar(missing_stats['entity'], missing_stats['completeness_pct'])\n",
    "axes[1, 1].set_title('Compl√©tude des donn√©es par entit√© (%)')\n",
    "axes[1, 1].set_ylabel('Pourcentage de donn√©es valides')\n",
    "axes[1, 1].set_xlabel('Entit√©')\n",
    "axes[1, 1].set_ylim(0, 100)\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar, value in zip(bars, missing_stats['completeness_pct']):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1,\n",
    "                   f'{value:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gznjj08803h",
   "metadata": {},
   "source": [
    "## D√©monstration des classes de validation crois√©e pour s√©ries temporelles\n",
    "\n",
    "Les classes `TSOutOfSampleSplit` et `TSInSampleSplit` sont con√ßues pour les s√©ries temporelles. Elles respectent l'ordre temporel et permettent diff√©rentes configurations selon les besoins d'√©valuation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "un6pkoksgv",
   "metadata": {},
   "source": [
    "### 3.1 TSOutOfSampleSplit - Validation hors √©chantillon\n",
    "\n",
    "La validation **out-of-sample** (hors √©chantillon) est la m√©thode standard pour √©valuer les mod√®les de pr√©vision en simulant leur comportement en pseudo temps-r√©el. L'entra√Ænement se fait **strictement sur le pass√©** et le test sur le **futur**, respectant ainsi l'ordre temporel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0jluy1h3ywph",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction d'analyse des caract√©ristiques d'une sp√©ration entre indices d'entra√Ænement et de test\n",
    "def analyze_split_characteristics(X, train_indices, test_indices, split_name):\n",
    "    \"\"\"Fonction utilitaire pour analyser les caract√©ristiques d'une s√©paration.\"\"\"\n",
    "    train_dates = X.index[train_indices]\n",
    "    test_dates = X.index[test_indices]\n",
    "    \n",
    "    print(f\"\\nüìä {split_name}:\")\n",
    "    print(f\"  - Taille d'entra√Ænement: {len(train_indices)} observations\")\n",
    "    print(f\"  - Taille de test: {len(test_indices)} observations\")\n",
    "    print(f\"  - P√©riode d'entra√Ænement: {train_dates[0].date()} √† {train_dates[-1].date()}\")\n",
    "    print(f\"  - P√©riode de test: {test_dates[0].date()} √† {test_dates[-1].date()}\")\n",
    "    \n",
    "    # V√©rification de l'ordre temporel\n",
    "    gap_days = (test_dates[0] - train_dates[-1]).days\n",
    "    print(f\"  - Gap entre train et test: {gap_days} jours\")\n",
    "    \n",
    "    return {\n",
    "        'train_size': len(train_indices),\n",
    "        'test_size': len(test_indices),\n",
    "        'train_start': train_dates[0],\n",
    "        'train_end': train_dates[-1],\n",
    "        'test_start': test_dates[0],\n",
    "        'test_end': test_dates[-1],\n",
    "        'gap_days': gap_days\n",
    "    }\n",
    "\n",
    "print(\"üîç D√âMONSTRATION: TSOutOfSampleSplit avec diff√©rents param√®tres\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Utilisation de la s√©rie avec trend pour les d√©monstrations\n",
    "X = ts_trend.to_frame('value')\n",
    "print(f\"S√©rie utilis√©e: {len(X)} observations de {X.index[0].date()} √† {X.index[-1].date()}\")\n",
    "\n",
    "# Configuration 1: Split basique avec n_splits\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 1: Split basique avec n_splits\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "splitter1 = TSOutOfSampleSplit(n_splits=3, test_size=20)\n",
    "splits_info = []\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splitter1.split(X)):\n",
    "    split_info = analyze_split_characteristics(X, train_idx, test_idx, f\"Split {i+1}\")\n",
    "    splits_info.append(split_info)\n",
    "\n",
    "# Configuration 2: Avec gap pour √©viter le data leakage\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 2: Avec gap pour √©viter le data leakage\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "splitter2 = TSOutOfSampleSplit(n_splits=3, test_size=15, gap=5)\n",
    "print(\"‚ö†Ô∏è  Gap = 5 mois entre l'entra√Ænement et le test\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splitter2.split(X)):\n",
    "    analyze_split_characteristics(X, train_idx, test_idx, f\"Split avec gap {i+1}\")\n",
    "\n",
    "# Configuration 3: Fen√™tre d'entra√Ænement limit√©e (rolling window)\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 3: Fen√™tre d'entra√Ænement limit√©e (rolling window)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "splitter3 = TSOutOfSampleSplit(n_splits=3, test_size=15, max_train_size=50, gap=2)\n",
    "print(\"üìè max_train_size = 50 observations (fen√™tre glissante)\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splitter3.split(X)):\n",
    "    analyze_split_characteristics(X, train_idx, test_idx, f\"Rolling window {i+1}\")\n",
    "\n",
    "# Configuration 4: Test sur des dates sp√©cifiques\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 4: Test sur des dates sp√©cifiques\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "specific_test_dates = ['2020-03-01', '2020-04-01', '2020-06-01']\n",
    "splitter4 = TSOutOfSampleSplit(test_indices=specific_test_dates, test_size=10, gap=3)\n",
    "print(f\"üéØ Dates de test sp√©cifiques: {specific_test_dates}\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(splitter4.split(X)):\n",
    "    analyze_split_characteristics(X, train_idx, test_idx, f\"Test sp√©cifique {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_trend.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jl0n1mg5qmc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des diff√©rentes configurations de TSOutOfSampleSplit\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Visualisation des diff√©rentes configurations TSOutOfSampleSplit', fontsize=16)\n",
    "\n",
    "configurations = [\n",
    "    (splitter1, \"Split basique (n_splits=3)\", axes[0, 0]),\n",
    "    (splitter2, \"Avec gap=5\", axes[0, 1]), \n",
    "    (splitter3, \"Fen√™tre limit√©e (max_train_size=50)\", axes[1, 0]),\n",
    "    (splitter4, \"Dates sp√©cifiques\", axes[1, 1])\n",
    "]\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "\n",
    "for splitter, title, ax in configurations:\n",
    "    # Plot de la s√©rie compl√®te\n",
    "    ax.plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n",
    "    \n",
    "    # Plot des splits\n",
    "    for i, (train_idx, test_idx) in enumerate(splitter.split(X)):\n",
    "        train_data = X.iloc[train_idx]\n",
    "        test_data = X.iloc[test_idx]\n",
    "        \n",
    "        # Donn√©es d'entra√Ænement\n",
    "        ax.plot(train_data.index, train_data['value'], \n",
    "               color=colors[i], alpha=0.7, linewidth=2, \n",
    "               label=f'Train {i+1}' if i < 3 else None)\n",
    "        \n",
    "        # Donn√©es de test\n",
    "        ax.scatter(test_data.index, test_data['value'], \n",
    "                  color=colors[i], s=30, alpha=0.9, marker='o',\n",
    "                  edgecolors='black', linewidth=0.5,\n",
    "                  label=f'Test {i+1}' if i < 3 else None)\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_ylabel('Valeur')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    if ax in [axes[1, 0], axes[1, 1]]:\n",
    "        ax.set_xlabel('Date')\n",
    "    \n",
    "    # L√©gende seulement pour le premier graphique\n",
    "    if ax == axes[0, 0]:\n",
    "        ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# R√©sum√© des caract√©ristiques importantes\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã R√âSUM√â DES CARACT√âRISTIQUES IMPORTANTES\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\n‚úÖ Points cl√©s √† retenir:\")\n",
    "print(\"  1. Out-of-sample: l'entra√Ænement pr√©c√®de TOUJOURS le test temporellement\")\n",
    "print(\"  2. Gap: permet d'√©viter le data leakage en laissant un intervalle\")\n",
    "print(\"  3. max_train_size: limite la fen√™tre d'entra√Ænement (rolling window)\")\n",
    "print(\"  4. test_indices: permet de tester sur des p√©riodes sp√©cifiques\")\n",
    "print(\"  5. Les splits respectent l'ordre chronologique des donn√©es\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3jofqtfrcdk",
   "metadata": {},
   "source": [
    "### 3.2 TSInSampleSplit - Validation dans l'√©chantillon\n",
    "\n",
    "La validation **in-sample** (dans l'√©chantillon) inclut la p√©riode de test dans les donn√©es d'entra√Ænement. Cette approche est utile pour l'√©valuation historique et la calibration de mod√®les, o√π l'information future est disponible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tw8nthfoue",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç D√âMONSTRATION: TSInSampleSplit - Validation dans l'√©chantillon\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Fonction d'analyse d'une s√©paration in sample\n",
    "def analyze_insample_split(X, train_indices, test_indices, split_name):\n",
    "    \"\"\"Fonction pour analyser les splits in-sample.\"\"\"\n",
    "    train_dates = X.index[train_indices]\n",
    "    test_dates = X.index[test_indices]\n",
    "    \n",
    "    print(f\"\\nüìä {split_name}:\")\n",
    "    print(f\"  - Taille d'entra√Ænement: {len(train_indices)} observations\")\n",
    "    print(f\"  - Taille de test: {len(test_indices)} observations\")\n",
    "    print(f\"  - P√©riode d'entra√Ænement: {train_dates[0].date()} √† {train_dates[-1].date()}\")\n",
    "    print(f\"  - P√©riode de test: {test_dates[0].date()} √† {test_dates[-1].date()}\")\n",
    "    \n",
    "    # V√©rification que le test est inclus dans l'entra√Ænement\n",
    "    test_in_train = all(idx in train_indices for idx in test_indices)\n",
    "    print(f\"  - Test inclus dans train: {'‚úÖ Oui' if test_in_train else '‚ùå Non'}\")\n",
    "    \n",
    "    return test_in_train\n",
    "\n",
    "# Configuration 1: In-sample basique\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 1: In-sample basique\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "insample_splitter1 = TSInSampleSplit(test_size=20)\n",
    "print(\"üìñ Les donn√©es de test sont incluses dans l'entra√Ænement\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(insample_splitter1.split(X)):\n",
    "    analyze_insample_split(X, train_idx, test_idx, f\"In-sample split {i+1}\")\n",
    "\n",
    "# Configuration 2: In-sample avec fen√™tre d'entra√Ænement limit√©e\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 2: In-sample avec max_train_size\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "insample_splitter2 = TSInSampleSplit(test_size=15, max_train_size=80)\n",
    "print(\"üìè Entra√Ænement limit√© mais inclut toujours la p√©riode de test\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(insample_splitter2.split(X)):\n",
    "    analyze_insample_split(X, train_idx, test_idx, f\"Limited in-sample {i+1}\")\n",
    "\n",
    "# Configuration 3: In-sample sur dates sp√©cifiques\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 3: In-sample sur dates sp√©cifiques\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "specific_dates = ['2020-04-01']\n",
    "insample_splitter3 = TSInSampleSplit(test_indices=specific_dates, test_size=14)\n",
    "print(f\"üéØ Test sur p√©riode sp√©cifique: {specific_dates[0]} (14 jours)\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(insample_splitter3.split(X)):\n",
    "    analyze_insample_split(X, train_idx, test_idx, f\"Specific in-sample {i+1}\")\n",
    "\n",
    "# Configuration 4: Comparaison de plusieurs dates sp√©cifiques\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 4: Multiples dates sp√©cifiques\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "multiple_dates = ['2020-02-01', '2020-03-01', '2020-05-01']\n",
    "insample_splitter4 = TSInSampleSplit(test_indices=multiple_dates, test_size=7)\n",
    "print(f\"üéØ Tests sur: {multiple_dates}\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(insample_splitter4.split(X)):\n",
    "    analyze_insample_split(X, train_idx, test_idx, f\"Multiple dates {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "og19pb6ryk9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation comparative: Out-of-sample vs In-sample\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "fig.suptitle('Comparaison Out-of-sample vs In-sample', fontsize=16)\n",
    "\n",
    "# Donn√©es de test communes pour la comparaison\n",
    "test_date_comparison = '2020-04-01'\n",
    "test_size_comparison = 10\n",
    "\n",
    "# Out-of-sample\n",
    "out_splitter = TSOutOfSampleSplit(test_indices=[test_date_comparison], test_size=test_size_comparison, gap=2)\n",
    "in_splitter = TSInSampleSplit(test_indices=[test_date_comparison], test_size=test_size_comparison)\n",
    "\n",
    "# Visualisation Out-of-sample\n",
    "for train_idx, test_idx in out_splitter.split(X):\n",
    "    train_data = X.iloc[train_idx]\n",
    "    test_data = X.iloc[test_idx]\n",
    "    \n",
    "    axes[0, 0].plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n",
    "    axes[0, 0].plot(train_data.index, train_data['value'], color='blue', alpha=0.8, linewidth=2, label='Train')\n",
    "    axes[0, 0].scatter(test_data.index, test_data['value'], color='red', s=40, alpha=0.9, \n",
    "                      edgecolors='black', linewidth=0.5, label='Test')\n",
    "    \n",
    "    # Mise en √©vidence du gap\n",
    "    if len(train_data) > 0 and len(test_data) > 0:\n",
    "        gap_start = train_data.index[-1]\n",
    "        gap_end = test_data.index[0]\n",
    "        axes[0, 0].axvspan(gap_start, gap_end, alpha=0.3, color='yellow', label='Gap')\n",
    "\n",
    "axes[0, 0].set_title('Out-of-sample: Train ‚Üí Gap ‚Üí Test')\n",
    "axes[0, 0].set_ylabel('Valeur')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Visualisation In-sample\n",
    "for train_idx, test_idx in in_splitter.split(X):\n",
    "    train_data = X.iloc[train_idx]\n",
    "    test_data = X.iloc[test_idx]\n",
    "    \n",
    "    axes[0, 1].plot(X.index, X['value'], color='lightgray', alpha=0.5, linewidth=1, label='Donn√©es compl√®tes')\n",
    "    axes[0, 1].plot(train_data.index, train_data['value'], color='blue', alpha=0.8, linewidth=2, label='Train')\n",
    "    axes[0, 1].scatter(test_data.index, test_data['value'], color='red', s=40, alpha=0.9, \n",
    "                      edgecolors='black', linewidth=0.5, label='Test')\n",
    "    \n",
    "    # Mise en √©vidence de l'overlap\n",
    "    overlap_indices = np.intersect1d(train_idx, test_idx)\n",
    "    if len(overlap_indices) > 0:\n",
    "        overlap_data = X.iloc[overlap_indices]\n",
    "        axes[0, 1].scatter(overlap_data.index, overlap_data['value'], color='purple', s=60, \n",
    "                          alpha=0.7, marker='s', edgecolors='black', linewidth=1, label='Overlap (Test dans Train)')\n",
    "\n",
    "axes[0, 1].set_title('In-sample: Test inclus dans Train')\n",
    "axes[0, 1].set_ylabel('Valeur')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Graphique de distribution des erreurs simul√©es\n",
    "print(\"\\nüßÆ Simulation d'√©valuation avec diff√©rents mod√®les...\")\n",
    "\n",
    "# Simulation d'erreurs pour d√©montrer l'impact\n",
    "np.random.seed(42)\n",
    "n_simulations = 1000\n",
    "\n",
    "# Erreurs out-of-sample (plus r√©alistes)\n",
    "out_sample_errors = np.random.normal(0, 1.5, n_simulations)  # Plus d'incertitude\n",
    "\n",
    "# Erreurs in-sample (g√©n√©ralement plus faibles)\n",
    "in_sample_errors = np.random.normal(0, 0.8, n_simulations)   # Moins d'incertitude\n",
    "\n",
    "axes[1, 0].hist(out_sample_errors, bins=50, alpha=0.7, color='red', label='Out-of-sample', density=True)\n",
    "axes[1, 0].hist(in_sample_errors, bins=50, alpha=0.7, color='blue', label='In-sample', density=True)\n",
    "axes[1, 0].set_title('Distribution des erreurs de pr√©diction')\n",
    "axes[1, 0].set_xlabel('Erreur')\n",
    "axes[1, 0].set_ylabel('Densit√©')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Statistiques comparatives\n",
    "out_mae = np.mean(np.abs(out_sample_errors))\n",
    "in_mae = np.mean(np.abs(in_sample_errors))\n",
    "out_mse = np.mean(out_sample_errors**2)\n",
    "in_mse = np.mean(in_sample_errors**2)\n",
    "\n",
    "metrics = ['MAE', 'MSE']\n",
    "out_values = [out_mae, out_mse]\n",
    "in_values = [in_mae, in_mse]\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = axes[1, 1].bar(x - width/2, out_values, width, label='Out-of-sample', color='red', alpha=0.7)\n",
    "bars2 = axes[1, 1].bar(x + width/2, in_values, width, label='In-sample', color='blue', alpha=0.7)\n",
    "\n",
    "axes[1, 1].set_title('M√©triques d\\'erreur comparatives')\n",
    "axes[1, 1].set_ylabel('Valeur de l\\'erreur')\n",
    "axes[1, 1].set_xticks(x)\n",
    "axes[1, 1].set_xticklabels(metrics)\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Ajout des valeurs sur les barres\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom')\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2., height + height*0.01,\n",
    "                   f'{height:.3f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìã COMPARAISON OUT-OF-SAMPLE vs IN-SAMPLE\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nüìä R√©sultats simul√©s:\")\n",
    "print(f\"  Out-of-sample MAE: {out_mae:.3f}\")\n",
    "print(f\"  In-sample MAE: {in_mae:.3f}\")\n",
    "print(f\"  Diff√©rence: {((out_mae - in_mae) / in_mae * 100):+.1f}%\")\n",
    "print(\"\\n‚úÖ Points cl√©s:\")\n",
    "print(\"  1. Out-of-sample: √©valuation r√©aliste de la capacit√© pr√©dictive\")\n",
    "print(\"  2. In-sample: √©valuation optimiste, utile pour la calibration\")\n",
    "print(\"  3. L'√©cart refl√®te le challenge r√©el de la pr√©diction\")\n",
    "print(\"  4. In-sample inclut information future ‚Üí erreurs plus faibles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tdqht99wpn",
   "metadata": {},
   "source": [
    "## D√©monstration des classes de validation crois√©e pour donn√©es de panel\n",
    "\n",
    "Les donn√©es de panel combinent plusieurs entit√©s observ√©es dans le temps. Les classes `PanelOutOfSampleSplit` et `PanelInSampleSplit` g√®rent cette complexit√© en appliquant la logique de validation √† chaque entit√© tout en permettant l'agr√©gation des r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fxbvksunyn",
   "metadata": {},
   "source": [
    "### 4.1 PanelOutOfSampleSplit - Validation hors √©chantillon pour donn√©es de panel\n",
    "\n",
    "Cette classe applique la logique out-of-sample √† chaque entit√© du panel, respectant l'ordre temporel au sein de chaque entit√©."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7zxxnn6b3r",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour analyser les s√©parations d'indices d'entra√Ænement et de test sur des donn√©es de panel\n",
    "def analyze_panel_split(X, train_indices, test_indices, split_name, max_entities_display=5):\n",
    "    \"\"\"Fonction pour analyser les splits de panel.\"\"\"\n",
    "    # Extraction des entit√©s pr√©sentes dans train et test\n",
    "    train_entities = X.iloc[train_indices].index.get_level_values('entity').unique()\n",
    "    test_entities = X.iloc[test_indices].index.get_level_values('entity').unique()\n",
    "    \n",
    "    print(f\"\\nüìä {split_name}:\")\n",
    "    print(f\"  - Observations d'entra√Ænement: {len(train_indices)}\")\n",
    "    print(f\"  - Observations de test: {len(test_indices)}\")\n",
    "    print(f\"  - Entit√©s dans train: {len(train_entities)} {list(train_entities[:max_entities_display])}\")\n",
    "    print(f\"  - Entit√©s dans test: {len(test_entities)} {list(test_entities[:max_entities_display])}\")\n",
    "    \n",
    "    # Analyse par entit√©\n",
    "    if len(test_entities) <= max_entities_display:\n",
    "        for entity in test_entities:\n",
    "            entity_train = X.iloc[train_indices].xs(entity, level='entity', drop_level=False)\n",
    "            entity_test = X.iloc[test_indices].xs(entity, level='entity', drop_level=False)\n",
    "            \n",
    "            if len(entity_train) > 0 and len(entity_test) > 0:\n",
    "                train_dates = entity_train.index.get_level_values('date')\n",
    "                test_dates = entity_test.index.get_level_values('date')\n",
    "                gap_days = (test_dates[0] - train_dates[-1]).days\n",
    "                print(f\"    {entity}: Train {train_dates[0].date()}‚Üí{train_dates[-1].date()}, Test {test_dates[0].date()}‚Üí{test_dates[-1].date()}, Gap {gap_days}j\")\n",
    "\n",
    "print(\"üîç D√âMONSTRATION: PanelOutOfSampleSplit\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Utilisation du panel √©quilibr√© pour les d√©monstrations\n",
    "X_panel = panel_balanced[['value']]\n",
    "print(f\"Panel utilis√©: {X_panel.shape[0]} observations, {len(X_panel.index.get_level_values('entity').unique())} entit√©s\")\n",
    "print(f\"P√©riode: {X_panel.index.get_level_values('date').min().date()} √† {X_panel.index.get_level_values('date').max().date()}\")\n",
    "\n",
    "# Configuration 1: Split basique avec n_splits\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 1: Panel out-of-sample basique\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "panel_splitter1 = PanelOutOfSampleSplit(n_splits=3, test_size=10)\n",
    "print(\"üìä Validation crois√©e avec 3 splits, 10 observations de test par entit√©\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter1.split(X_panel)):\n",
    "    analyze_panel_split(X_panel, train_idx, test_idx, f\"Panel split {i+1}\")\n",
    "    if i >= 2:  # Limiter l'affichage\n",
    "        break\n",
    "\n",
    "# Configuration 2: Avec gap\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 2: Panel avec gap\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "panel_splitter2 = PanelOutOfSampleSplit(n_splits=2, test_size=8, gap=5)\n",
    "print(\"‚ö†Ô∏è  Gap de 5 jours entre train et test pour chaque entit√©\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter2.split(X_panel)):\n",
    "    analyze_panel_split(X_panel, train_idx, test_idx, f\"Panel avec gap {i+1}\")\n",
    "\n",
    "# Configuration 3: Test sur dates sp√©cifiques\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 3: Test sur dates sp√©cifiques (panel)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "specific_panel_dates = ['2020-03-01', '2020-04-01']\n",
    "panel_splitter3 = PanelOutOfSampleSplit(test_indices=specific_panel_dates, test_size=7, gap=2)\n",
    "print(f\"üéØ Tests sur: {specific_panel_dates} pour toutes les entit√©s\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter3.split(X_panel)):\n",
    "    analyze_panel_split(X_panel, train_idx, test_idx, f\"Dates sp√©cifiques {i+1}\")\n",
    "\n",
    "# Configuration 4: Fen√™tre d'entra√Ænement limit√©e\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"CONFIGURATION 4: Fen√™tre d'entra√Ænement limit√©e (panel)\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "panel_splitter4 = PanelOutOfSampleSplit(n_splits=2, test_size=6, max_train_size=30, gap=1)\n",
    "print(\"üìè max_train_size = 30 observations par entit√©\")\n",
    "\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter4.split(X_panel)):\n",
    "    analyze_panel_split(X_panel, train_idx, test_idx, f\"Rolling window panel {i+1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h67e6ffpqd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des splits de panel\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Visualisation des splits PanelOutOfSampleSplit', fontsize=16)\n",
    "\n",
    "# Configuration pour la visualisation\n",
    "entities_to_plot = entities_small[:3]  # Premi√®re 3 entit√©s pour la clart√©\n",
    "colors_entities = ['blue', 'red', 'green']\n",
    "\n",
    "# Configuration 1: Split basique\n",
    "ax = axes[0, 0]\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter1.split(X_panel)):\n",
    "    if i > 0:  # Seulement le premier split pour la clart√©\n",
    "        break\n",
    "    \n",
    "    for j, entity in enumerate(entities_to_plot):\n",
    "        try:\n",
    "            # Donn√©es compl√®tes de l'entit√©\n",
    "            entity_data = X_panel.xs(entity, level='entity')\n",
    "            ax.plot(entity_data.index, entity_data['value'], \n",
    "                   color=colors_entities[j], alpha=0.3, linewidth=1, \n",
    "                   label=f'{entity} (complet)' if i == 0 else \"\")\n",
    "            \n",
    "            # Donn√©es d'entra√Ænement\n",
    "            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_train_data) > 0:\n",
    "                train_dates = entity_train_data.index.get_level_values('date')\n",
    "                ax.plot(train_dates, entity_train_data['value'], \n",
    "                       color=colors_entities[j], alpha=0.8, linewidth=2,\n",
    "                       label=f'{entity} Train' if i == 0 else \"\")\n",
    "            \n",
    "            # Donn√©es de test\n",
    "            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_test_data) > 0:\n",
    "                test_dates = entity_test_data.index.get_level_values('date')\n",
    "                ax.scatter(test_dates, entity_test_data['value'], \n",
    "                          color=colors_entities[j], s=40, alpha=0.9,\n",
    "                          edgecolors='black', linewidth=0.5,\n",
    "                          label=f'{entity} Test' if i == 0 else \"\")\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "ax.set_title('Panel split basique')\n",
    "ax.set_ylabel('Valeur')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=8)\n",
    "\n",
    "# Configuration 2: Avec gap\n",
    "ax = axes[0, 1]\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter2.split(X_panel)):\n",
    "    if i > 0:  # Seulement le premier split\n",
    "        break\n",
    "    \n",
    "    for j, entity in enumerate(entities_to_plot):\n",
    "        try:\n",
    "            entity_data = X_panel.xs(entity, level='entity')\n",
    "            ax.plot(entity_data.index, entity_data['value'], \n",
    "                   color=colors_entities[j], alpha=0.3, linewidth=1)\n",
    "            \n",
    "            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_train_data) > 0:\n",
    "                train_dates = entity_train_data.index.get_level_values('date')\n",
    "                ax.plot(train_dates, entity_train_data['value'], \n",
    "                       color=colors_entities[j], alpha=0.8, linewidth=2)\n",
    "            \n",
    "            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_test_data) > 0:\n",
    "                test_dates = entity_test_data.index.get_level_values('date')\n",
    "                ax.scatter(test_dates, entity_test_data['value'], \n",
    "                          color=colors_entities[j], s=40, alpha=0.9,\n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "                \n",
    "                # Visualisation du gap\n",
    "                if len(entity_train_data) > 0:\n",
    "                    gap_start = train_dates[-1]\n",
    "                    gap_end = test_dates[0]\n",
    "                    ax.axvspan(gap_start, gap_end, alpha=0.2, color='yellow')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "ax.set_title('Panel avec gap=5')\n",
    "ax.set_ylabel('Valeur')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Configuration 3: Dates sp√©cifiques\n",
    "ax = axes[1, 0]\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter3.split(X_panel)):\n",
    "    if i > 1:  # Limite √† 2 splits\n",
    "        break\n",
    "    \n",
    "    for j, entity in enumerate(entities_to_plot):\n",
    "        try:\n",
    "            entity_data = X_panel.xs(entity, level='entity')\n",
    "            ax.plot(entity_data.index, entity_data['value'], \n",
    "                   color=colors_entities[j], alpha=0.3, linewidth=1)\n",
    "            \n",
    "            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_train_data) > 0:\n",
    "                train_dates = entity_train_data.index.get_level_values('date')\n",
    "                ax.plot(train_dates, entity_train_data['value'], \n",
    "                       color=colors_entities[j], alpha=0.8, linewidth=2)\n",
    "            \n",
    "            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_test_data) > 0:\n",
    "                test_dates = entity_test_data.index.get_level_values('date')\n",
    "                ax.scatter(test_dates, entity_test_data['value'], \n",
    "                          color=colors_entities[j], s=40, alpha=0.9,\n",
    "                          edgecolors='black', linewidth=0.5,\n",
    "                          marker='s' if i == 0 else 'o')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "ax.set_title('Dates sp√©cifiques')\n",
    "ax.set_ylabel('Valeur')\n",
    "ax.set_xlabel('Date')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Configuration 4: Fen√™tre limit√©e\n",
    "ax = axes[1, 1]\n",
    "for i, (train_idx, test_idx) in enumerate(panel_splitter4.split(X_panel)):\n",
    "    if i > 0:  # Seulement le premier split\n",
    "        break\n",
    "    \n",
    "    for j, entity in enumerate(entities_to_plot):\n",
    "        try:\n",
    "            entity_data = X_panel.xs(entity, level='entity')\n",
    "            ax.plot(entity_data.index, entity_data['value'], \n",
    "                   color=colors_entities[j], alpha=0.3, linewidth=1)\n",
    "            \n",
    "            entity_train_data = X_panel.iloc[train_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_train_data) > 0:\n",
    "                train_dates = entity_train_data.index.get_level_values('date')\n",
    "                ax.plot(train_dates, entity_train_data['value'], \n",
    "                       color=colors_entities[j], alpha=0.8, linewidth=2)\n",
    "            \n",
    "            entity_test_data = X_panel.iloc[test_idx].xs(entity, level='entity', drop_level=False)\n",
    "            if len(entity_test_data) > 0:\n",
    "                test_dates = entity_test_data.index.get_level_values('date')\n",
    "                ax.scatter(test_dates, entity_test_data['value'], \n",
    "                          color=colors_entities[j], s=40, alpha=0.9,\n",
    "                          edgecolors='black', linewidth=0.5)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "ax.set_title('Fen√™tre limit√©e (max_train_size=30)')\n",
    "ax.set_ylabel('Valeur')\n",
    "ax.set_xlabel('Date')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zvm4mjf6rxq",
   "metadata": {},
   "source": [
    "### 4.2 Classes sp√©cialis√©es pour le traitement par entit√©\n",
    "\n",
    "Les classes `PanelOutOfSampleSplitPerEntity` et `PanelInSampleSplitPerEntity` permettent de traiter chaque entit√© s√©par√©ment, ce qui est utile pour l'analyse individuelle et le traitement parall√®le."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13yshjeq25zq",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üîç D√âMONSTRATION: Classes sp√©cialis√©es par entit√©\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Comparaison des approches PanelOutOfSampleSplit vs PanelOutOfSampleSplitPerEntity\n",
    "print(\"\\nüìä Comparaison: Agr√©g√© vs Par entit√©\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Splitter agr√©g√© (standard)\n",
    "panel_agg_splitter = PanelOutOfSampleSplit(n_splits=2, test_size=5, gap=2)\n",
    "\n",
    "# Splitter par entit√©\n",
    "panel_per_entity_splitter = PanelOutOfSampleSplitPerEntity(n_splits=2, test_size=5, gap=2)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  APPROCHE AGR√âG√âE (PanelOutOfSampleSplit):\")\n",
    "split_count = 0\n",
    "# Parcours des s√©parations\n",
    "for train_idx, test_idx in panel_agg_splitter.split(X_panel):\n",
    "    split_count += 1\n",
    "    entities_in_test = X_panel.iloc[test_idx].index.get_level_values('entity').unique()\n",
    "    print(f\"  Split {split_count}: {len(test_idx)} observations de test, {len(entities_in_test)} entit√©s\")\n",
    "    if split_count >= 2:\n",
    "        break\n",
    "\n",
    "print(f\"\\n  üìà Total: {split_count} splits avec toutes les entit√©s m√©lang√©es\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  APPROCHE PAR ENTIT√â (PanelOutOfSampleSplitPerEntity):\")\n",
    "split_count = 0\n",
    "entity_splits = {}\n",
    "\n",
    "# Parcours des s√©parations\n",
    "for train_idx, test_idx in panel_per_entity_splitter.split(X_panel):\n",
    "    split_count += 1\n",
    "    # Identifier l'entit√© de ce split\n",
    "    test_entity = X_panel.iloc[test_idx].index.get_level_values('entity').unique()[0]\n",
    "    train_entity = X_panel.iloc[train_idx].index.get_level_values('entity').unique()[0] if len(train_idx) > 0 else \"N/A\"\n",
    "    \n",
    "    if test_entity not in entity_splits:\n",
    "        entity_splits[test_entity] = 0\n",
    "    entity_splits[test_entity] += 1\n",
    "    \n",
    "    print(f\"  Split {split_count}: Entit√© {test_entity}, {len(train_idx)} train, {len(test_idx)} test\")\n",
    "    \n",
    "    if split_count >= 8:  # Limiter l'affichage\n",
    "        print(\"  ... (splits suppl√©mentaires)\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n  üìà Total: {split_count}+ splits individuels par entit√©\")\n",
    "print(f\"  üìä R√©partition par entit√©: {dict(entity_splits)}\")\n",
    "\n",
    "# D√©monstration avec In-sample per entity\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"D√âMONSTRATION: PanelInSampleSplitPerEntity\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "panel_insample_per_entity = PanelInSampleSplitPerEntity(test_indices=['2020-03-01'], test_size=7)\n",
    "\n",
    "print(\"üéØ Test sur 2020-03-01 avec validation in-sample par entit√©:\")\n",
    "entity_results = {}\n",
    "\n",
    "# Parcours des s√©parations\n",
    "for train_idx, test_idx in panel_insample_per_entity.split(X_panel):\n",
    "    test_entity = X_panel.iloc[test_idx].index.get_level_values('entity').unique()[0]\n",
    "    train_data = X_panel.iloc[train_idx]\n",
    "    test_data = X_panel.iloc[test_idx]\n",
    "    \n",
    "    # V√©rification que le test est inclus dans l'entra√Ænement\n",
    "    test_in_train = all(idx in train_idx for idx in test_idx)\n",
    "    \n",
    "    entity_results[test_entity] = {\n",
    "        'train_size': len(train_idx),\n",
    "        'test_size': len(test_idx),\n",
    "        'test_in_train': test_in_train,\n",
    "        'train_period': (train_data.index.get_level_values('date').min().date(), \n",
    "                        train_data.index.get_level_values('date').max().date()),\n",
    "        'test_period': (test_data.index.get_level_values('date').min().date(),\n",
    "                       test_data.index.get_level_values('date').max().date())\n",
    "    }\n",
    "    \n",
    "    print(f\"  {test_entity}: Train {entity_results[test_entity]['train_size']} obs, \"\n",
    "          f\"Test {entity_results[test_entity]['test_size']} obs, \"\n",
    "          f\"Test in Train: {'‚úÖ' if test_in_train else '‚ùå'}\")\n",
    "\n",
    "# Statistiques de comparaison\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"üìä STATISTIQUES COMPARATIVES\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "def calculate_split_statistics(splitter, X, split_type_name):\n",
    "    \"\"\"Calcule des statistiques sur les splits.\"\"\"\n",
    "    total_splits = 0\n",
    "    total_train_obs = 0\n",
    "    total_test_obs = 0\n",
    "    entities_seen = set()\n",
    "    \n",
    "    for train_idx, test_idx in splitter.split(X):\n",
    "        total_splits += 1\n",
    "        total_train_obs += len(train_idx)\n",
    "        total_test_obs += len(test_idx)\n",
    "        \n",
    "        test_entities = X.iloc[test_idx].index.get_level_values('entity').unique()\n",
    "        entities_seen.update(test_entities)\n",
    "        \n",
    "        if total_splits >= 10:  # Limite pour √©viter trop de calculs\n",
    "            break\n",
    "    \n",
    "    return {\n",
    "        'type': split_type_name,\n",
    "        'total_splits': total_splits,\n",
    "        'avg_train_size': total_train_obs / total_splits if total_splits > 0 else 0,\n",
    "        'avg_test_size': total_test_obs / total_splits if total_splits > 0 else 0,\n",
    "        'unique_entities': len(entities_seen)\n",
    "    }\n",
    "\n",
    "# Calcul des statistiques\n",
    "stats_agg = calculate_split_statistics(panel_agg_splitter, X_panel, \"Agr√©g√©\")\n",
    "stats_per_entity = calculate_split_statistics(panel_per_entity_splitter, X_panel, \"Par entit√©\")\n",
    "\n",
    "print(f\"\\nüìà R√©sultats (sur {min(10, stats_agg['total_splits'])} premiers splits):\")\n",
    "print(f\"  Approche agr√©g√©e:\")\n",
    "print(f\"    - Splits: {stats_agg['total_splits']}\")\n",
    "print(f\"    - Taille moyenne train: {stats_agg['avg_train_size']:.1f}\")\n",
    "print(f\"    - Taille moyenne test: {stats_agg['avg_test_size']:.1f}\")\n",
    "print(f\"    - Entit√©s uniques vues: {stats_agg['unique_entities']}\")\n",
    "\n",
    "print(f\"  Approche par entit√©:\")\n",
    "print(f\"    - Splits: {stats_per_entity['total_splits']}\")\n",
    "print(f\"    - Taille moyenne train: {stats_per_entity['avg_train_size']:.1f}\")\n",
    "print(f\"    - Taille moyenne test: {stats_per_entity['avg_test_size']:.1f}\")\n",
    "print(f\"    - Entit√©s uniques vues: {stats_per_entity['unique_entities']}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Avantages par approche:\")\n",
    "print(f\"  üìä Agr√©g√©e: Moins de splits, √©valuation globale, plus rapide\")\n",
    "print(f\"  üéØ Par entit√©: Analyse d√©taill√©e, traitement parall√®le possible, contr√¥le granulaire\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6v7uwfe7pyg",
   "metadata": {},
   "source": [
    "## üìö Guide Pratique : Choisir la bonne m√©thode de validation crois√©e\n",
    "\n",
    "Cette section pr√©sente un guide pratique pour choisir la m√©thode de validation crois√©e appropri√©e selon diff√©rents sc√©narios et objectifs d'analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hcgbssic1li",
   "metadata": {},
   "source": [
    "### üéØ Matrice de d√©cision\n",
    "\n",
    "#### Type de donn√©es\n",
    "- **S√©rie temporelle unique** : `TSOutOfSampleSplit`, `TSInSampleSplit`\n",
    "- **Panel data (multi-entit√©s)** : `PanelOutOfSampleSplit`, `PanelInSampleSplit`, `PanelOutOfSampleSplitPerEntity`, `PanelInSampleSplitPerEntity`\n",
    "\n",
    "#### Objectif d'√©valuation\n",
    "- **Pr√©diction future (production)** : `OutOfSampleSplit`\n",
    "- **Analyse historique/calibration** : `InSampleSplit`\n",
    "- **√âvaluation comparative** : `OutOfSampleSplit`, `InSampleSplit`\n",
    "\n",
    "#### Contraintes temporelles\n",
    "- **√âviter data leakage** : `gap > 0`\n",
    "- **Fen√™tre glissante** : `max_train_size < inf`\n",
    "- **Dates sp√©cifiques** : `test_indices=[dates]`\n",
    "\n",
    "#### Analyse par entit√©\n",
    "- **√âvaluation globale** : `PanelOutOfSampleSplit`, `PanelInSampleSplit`\n",
    "- **Analyse individuelle** : `PanelOutOfSampleSplitPerEntity`, `PanelInSampleSplitPerEntity`\n",
    "- **Traitement parall√®le** : Classes `PerEntity`\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Configurations recommand√©es pour l'√©conomie et finances publiques\n",
    "\n",
    "#### 1Ô∏è‚É£ Pr√©diction du PIB trimestriel\n",
    "```python\n",
    "TSOutOfSampleSplit(n_splits=8, test_size=4, gap=1)\n",
    "```\n",
    "**Rationale** : Le gap vaut de mani√®re g√©n√©rale `max(horizon de pr√©diction, d√©lai de plublication de la variable √† pr√©dire)`. Dans le cas de la pr√©diction pour le trimestre suivant, on utilise ici un gap d'1 trimestre pour √©viter les r√©visions de donn√©es, test sur 1 ann√©e compl√®te\n",
    "\n",
    "#### 2Ô∏è‚É£ √âvaluation de mod√®les sur panel de pays OCDE\n",
    "```python\n",
    "PanelOutOfSampleSplit(test_size=12, gap=2, max_train_size=60)\n",
    "```\n",
    "**Rationale** : Fen√™tre de 5 ans, gap de 6 mois, test sur 3 ann√©es (donn√©es trimestrielles)\n",
    "\n",
    "#### 3Ô∏è‚É£ Backtesting de crise financi√®re (2008, COVID)\n",
    "```python\n",
    "TSInSampleSplit(test_indices=['2008-Q3', '2020-Q1'], test_size=8)\n",
    "```\n",
    "**Rationale** : Test sur p√©riodes de crise sp√©cifiques, entra√Ænement inclut les donn√©es post-crise\n",
    "\n",
    "#### 4Ô∏è‚É£ Analyse de robustesse des pr√©visions budg√©taires par minist√®re\n",
    "```python\n",
    "PanelOutOfSampleSplitPerEntity(n_splits=5, test_size=6)\n",
    "```\n",
    "**Rationale** : √âvaluation individuelle de chaque minist√®re avec 5 p√©riodes de test semestrielles\n",
    "\n",
    "#### 5Ô∏è‚É£ Pr√©diction de recettes fiscales mensuelles\n",
    "```python\n",
    "TSOutOfSampleSplit(test_size=6, gap=3, max_train_size=36)\n",
    "```\n",
    "**Rationale** : Gap important pour les retards de collecte, fen√™tre de 3 ans pour capturer la saisonnalit√©\n",
    "\n",
    "#### 6Ô∏è‚É£ Mod√©lisation de la dette souveraine par zone √©conomique\n",
    "```python\n",
    "PanelOutOfSampleSplit(test_indices=['2010-Q1', '2015-Q1', '2020-Q1'], test_size=12)\n",
    "```\n",
    "**Rationale** : Test sur p√©riodes cl√©s (crise grecque, normalisation QE, COVID), horizon 3 ans\n",
    "\n",
    "---\n",
    "\n",
    "### üî¨ M√©triques d'√©valuation typiques\n",
    "\n",
    "| Contexte | M√©triques recommand√©es | Horizon optimal |\n",
    "|----------|----------------------|-----------------|\n",
    "| **PIB** | RMSE, MAPE, Direction Accuracy | 1-4 trimestres |\n",
    "| **Inflation** | RMSE, MAE sur variations | 6-12 mois |\n",
    "| **Finances publiques** | MAPE, Bias, Coverage Ratio | 1-3 ann√©es |\n",
    "| **Taux de change** | RMSE, Sign Accuracy | 1-12 mois |\n",
    "| **Dette/PIB** | MAE, Stress Test Coverage | 3-10 ans |\n",
    "\n",
    "---\n",
    "\n",
    "### üìä Exemple de pipeline d'√©valuation\n",
    "\n",
    "```python\n",
    "def evaluate_macro_model(X, splitter, model, metric='rmse'):\n",
    "    \"\"\"\n",
    "    √âvalue un mod√®le macro-√©conomique avec validation crois√©e temporelle.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : pd.DataFrame\n",
    "        Donn√©es macro-√©conomiques avec index temporel\n",
    "    splitter : CrossValidator\n",
    "        M√©thode de validation crois√©e temporelle\n",
    "    model : estimator\n",
    "        Mod√®le √† √©valuer (sklearn-compatible)\n",
    "    metric : str\n",
    "        M√©trique d'√©valuation ('rmse', 'mape', 'direction')\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : R√©sultats d'√©valuation avec statistiques\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, test_idx in splitter.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = X_train['target'], X_test['target']\n",
    "        \n",
    "        # Entra√Ænement\n",
    "        model.fit(X_train.drop('target', axis=1), y_train)\n",
    "        \n",
    "        # Pr√©diction\n",
    "        y_pred = model.predict(X_test.drop('target', axis=1))\n",
    "        \n",
    "        # Calcul de la m√©trique\n",
    "        if metric == 'rmse':\n",
    "            score = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        elif metric == 'mape':\n",
    "            score = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "        elif metric == 'direction':\n",
    "            score = np.mean(np.sign(y_test.diff()) == np.sign(pd.Series(y_pred).diff()))\n",
    "        \n",
    "        scores.append(score)\n",
    "    \n",
    "    return {\n",
    "        'mean': np.mean(scores),\n",
    "        'std': np.std(scores),\n",
    "        'scores': scores,\n",
    "        'n_splits': len(scores)\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Points cl√©s √† retenir\n",
    "\n",
    "1. **Out-of-sample** donne une √©valuation plus conservative et r√©aliste pour la production\n",
    "2. **In-sample** est utile pour l'analyse historique et la validation de crises pass√©es\n",
    "3. **Le gap** pr√©vient le data leakage critique dans les donn√©es macro (r√©visions, retards de publication)\n",
    "4. **max_train_size** permet une validation en fen√™tre glissante adapt√©e aux changements structurels\n",
    "5. **Les classes Panel** g√®rent automatiquement les donn√©es multi-pays/multi-secteurs\n",
    "6. **PerEntity** permet l'analyse granulaire par juridiction et le traitement parall√®le\n",
    "7. **test_indices** permet de tester sur des √©v√©nements macro sp√©cifiques (crises, changements de politique)\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Recommandations par cas d'usage\n",
    "\n",
    "| **Objectif** | **M√©thode recommand√©e** | **Configuration type** |\n",
    "|--------------|------------------------|------------------------|\n",
    "| **üöÄ Production** | Out-of-sample avec gap appropri√© | `gap=2-3` p√©riodes selon fr√©quence |\n",
    "| **üìä Recherche** | In-sample pour analyse historique | `test_indices` sur √©v√©nements cl√©s |\n",
    "| **üî¨ Robustesse** | Comparaison des deux approches | M√™me configuration, m√©triques multiples |\n",
    "| **üåç Panel pays** | `PanelOutOfSampleSplit` | `max_train_size` pour √©viter sur-ajustement |\n",
    "| **üèõÔ∏è Analyse institutionnelle** | `PerEntity` classes | Traitement parall√®le par entit√© |\n",
    "\n",
    "---\n",
    "\n",
    "> **üí° Conseil pratique** : Commencez toujours par une validation out-of-sample pour √©tablir une baseline r√©aliste, puis utilisez l'in-sample pour l'analyse des m√©canismes et la calibration fine sur des p√©riodes historiques sp√©cifiques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84m3mwzzm",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Ce notebook a pr√©sent√© de mani√®re exhaustive les classes de validation crois√©e du module `tsforecast.crossvals`. Voici les points essentiels √† retenir :\n",
    "\n",
    "### Fonctionnalit√©s principales\n",
    "\n",
    "1. **Classes pour s√©ries temporelles** : `TSOutOfSampleSplit` et `TSInSampleSplit`\n",
    "   - Respectent l'ordre chronologique\n",
    "   - G√®rent les gaps pour √©viter le data leakage\n",
    "   - Supportent les fen√™tres glissantes et les dates sp√©cifiques\n",
    "\n",
    "2. **Classes pour donn√©es de panel** : `PanelOutOfSampleSplit`, `PanelInSampleSplit` et leurs variantes `PerEntity`\n",
    "   - Traitent les donn√©es multi-entit√©s automatiquement\n",
    "   - Appliquent la logique temporelle au sein de chaque entit√©\n",
    "   - Permettent l'analyse granulaire par entit√©\n",
    "\n",
    "3. **Flexibilit√© de configuration** : \n",
    "   - Param√®tres `n_splits`, `test_size`, `gap`, `max_train_size`\n",
    "   - Support des `test_indices` pour des p√©riodes sp√©cifiques\n",
    "   - Compatible avec l'API sklearn\n",
    "\n",
    "### Recommandations d'usage\n",
    "\n",
    "- **Out-of-sample** : Pour l'√©valuation r√©aliste de mod√®les de pr√©diction\n",
    "- **In-sample** : Pour l'analyse historique et la calibration\n",
    "- **PerEntity** : Pour l'analyse d√©taill√©e par entit√© et le traitement parall√®le\n",
    "\n",
    "Ces classes offrent une base pour l'√©valuation rigoureuse de mod√®les sur donn√©es temporelles et de panel, en respectant les contraintes inh√©rentes √† ce type de donn√©es."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts-forecast-YhvOQP7D-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
